{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda, Dropout\n",
    "from keras.layers import LeakyReLU, Multiply, add, dot, SeparableConv2D, DepthwiseConv2D, Reshape, Add, ReLU, Permute, Layer, InputSpec\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2  # number of classes\n",
    "img_width, img_height = 64, 64  # change based on the shape/structure of your images\n",
    "batch_size = 32  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 300  # number of iteration the algorithm gets trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/kim3/fake_detection/task2/128x128/train'\n",
    "validation_dir = '/home/kim3/fake_detection/task2/128x128/validation'\n",
    "test50_dir = '/home/kim3/fake_detection/task2/128x128/test_50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 11:26:41.226473 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0925 11:26:41.227696 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0925 11:26:41.231429 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0925 11:26:41.256895 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0925 11:26:41.258094 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0925 11:26:41.909147 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0925 11:26:42.367438 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 31, 31, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 31, 31, 32)   128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 31, 31, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 29, 29, 64)   18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 29, 29, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 29, 29, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 29, 29, 128)  8768        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 29, 29, 128)  512         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 29, 29, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 29, 29, 128)  17536       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 128)  8192        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 128)  0           separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 15, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 128)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 15, 15, 256)  33920       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 15, 256)  1024        separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 15, 15, 256)  67840       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 256)    32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 256)    1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 256)    0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 8, 8, 728)    188672      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 728)    2912        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 728)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 8, 8, 728)    536536      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 728)    186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 728)    0           separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4, 4, 728)    2912        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 728)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 4, 4, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 4, 4, 728)    536536      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 4, 4, 728)    2912        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 4, 4, 728)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 4, 4, 728)    536536      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 4, 4, 728)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 4, 4, 728)    536536      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 4, 4, 728)    0           batch_normalization_11[0][0]     \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 4, 4, 728)    536536      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 728)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 4, 4, 728)    536536      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 728)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 4, 4, 728)    536536      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 728)    0           batch_normalization_14[0][0]     \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 4, 4, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 4, 4, 728)    536536      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4, 4, 728)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 4, 4, 728)    536536      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4, 4, 728)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 4, 4, 728)    536536      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 728)    0           batch_normalization_17[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 4, 4, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 4, 4, 728)    536536      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 728)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 4, 4, 728)    536536      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 728)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_18 (SeparableC (None, 4, 4, 728)    536536      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 4, 4, 728)    0           batch_normalization_20[0][0]     \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_19 (SeparableC (None, 4, 4, 728)    536536      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 728)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_20 (SeparableC (None, 4, 4, 728)    536536      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_20[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 728)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_21 (SeparableC (None, 4, 4, 728)    536536      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 728)    0           batch_normalization_23[0][0]     \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_22 (SeparableC (None, 4, 4, 728)    536536      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 728)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_23 (SeparableC (None, 4, 4, 728)    536536      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 728)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_24 (SeparableC (None, 4, 4, 728)    536536      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 728)    0           batch_normalization_26[0][0]     \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_25 (SeparableC (None, 4, 4, 728)    536536      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 728)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_26 (SeparableC (None, 4, 4, 728)    536536      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 728)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_27 (SeparableC (None, 4, 4, 728)    536536      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 728)    0           batch_normalization_29[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_28 (SeparableC (None, 4, 4, 728)    536536      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 728)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_29 (SeparableC (None, 4, 4, 728)    536536      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 728)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_30 (SeparableC (None, 4, 4, 728)    536536      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 728)    0           batch_normalization_32[0][0]     \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 728)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_31 (SeparableC (None, 4, 4, 728)    536536      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 728)    2912        separable_conv2d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 728)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_32 (SeparableC (None, 4, 4, 1024)   752024      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 1024)   4096        separable_conv2d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 2, 2, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 1024)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 2, 1024)   4096        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 2, 2, 1024)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_33 (SeparableC (None, 2, 2, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 2, 2, 1536)   6144        separable_conv2d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 2, 2, 1536)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_34 (SeparableC (None, 2, 2, 2048)   3159552     activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 2, 2, 2048)   8192        separable_conv2d_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 2, 2, 2048)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            4098        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 20,861,130\n",
      "Trainable params: 20,808,826\n",
      "Non-trainable params: 52,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# layer 1 #\n",
    "x = Conv2D(filters=32, kernel_size=(3, 3), strides=2, padding='valid', use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# layer 2 #\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), padding='valid', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# skip layer 1 #\n",
    "res = Conv2D(filters=128, kernel_size=(1, 1), strides=2, padding='same', use_bias=False)(x)\n",
    "res = BatchNormalization()(res)\n",
    "\n",
    "# layer 3 #\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# layer 4 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3,3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "x = Add()([x, res])\n",
    "\n",
    "# skip layer 2 #\n",
    "res = Conv2D(filters=256, kernel_size=(1, 1), strides=2, padding='same', use_bias=False)(x)\n",
    "res = BatchNormalization()(res)\n",
    "\n",
    "# layer 5 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# layer 6 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3,3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "x = Add()([x, res])\n",
    "\n",
    "# skip layer 3 #\n",
    "res = Conv2D(filters=728, kernel_size=(1, 1), strides=2, padding='same', use_bias=False)(x)\n",
    "res = BatchNormalization()(res)\n",
    "\n",
    "# layer 7 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=728, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# layer 8 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=728, kernel_size=(3,3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "x = Add()([x, res])\n",
    "\n",
    "# ======== middle flow ========= #\n",
    "for i in range(8):\n",
    "    # layer 9, 10, 11, 12, 13, 14, 15, 16, 17 #\n",
    "    res = x\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(filters=728, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(filters=728, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)    \n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(filters=728, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Add()([x, res])    \n",
    "\n",
    "# ======== exit flow ========== #\n",
    "# skip layer 4 #\n",
    "res = Conv2D(filters=1024, kernel_size=(1, 1), strides=2, padding='same', use_bias=False)(x)\n",
    "res = BatchNormalization()(res)\n",
    "\n",
    "# layer 18 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=728, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# layer 19 #\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(filters=1024, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
    "x = Add()([x, res])\n",
    "\n",
    "# layer 20 #\n",
    "x = SeparableConv2D(filters=1536, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# layer 21 #\n",
    "x = SeparableConv2D(filters=2048, kernel_size=(3, 3), strides=1, padding='same', use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x_gap = GlobalAveragePooling2D()(x)\n",
    "output = Dense(units=2, activation='softmax')(x_gap)\n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 11:26:45.133799 139941363332864 deprecation_wrapper.py:119] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128404 images belonging to 2 classes.\n",
      "Found 32100 images belonging to 2 classes.\n",
      "Found 37566 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=20.0, \n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   rescale=1./255, )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 11:27:00.273358 139941363332864 deprecation.py:323] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/home/kim3/fake_detection/model/xception_94_96.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img):\n",
    "    \"\"\"\n",
    "    # Function: RandomCrop (ZeroPadded (4, 4)) + random occulusion image\n",
    "    # Arguments:\n",
    "        img: image\n",
    "    # Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]\n",
    "    MAX_CUTS = 3 # chance to get more cuts\n",
    "    MAX_LENGTH_MUTIPLIER = 5 # chance to get larger cuts\n",
    "    # 16 for cifar10, 8 for cifar100\n",
    "    \n",
    "    # Zero-padded (4, 4)\n",
    "#     img = np.pad(img, ((4,4),(4,4),(0,0)), mode='constant', constant_values=(0))\n",
    "    \n",
    "#     # random-crop 64x64\n",
    "#     dy, dx = height, width\n",
    "#     x = np.random.randint(0, width - dx + 1)\n",
    "#     y = np.random.randint(0, height - dy + 1)\n",
    "#     img = img[y:(y+dy), x:(x+dx)]\n",
    "    \n",
    "#     mean norm\n",
    "#     mean = img.mean(keepdims=True)\n",
    "#     img -= mean\n",
    "\n",
    "    img *= 1./255\n",
    "    \n",
    "    mask = np.ones((height, width, channels), dtype=np.float32)\n",
    "    nb_cuts = np.random.randint(0, MAX_CUTS + 1)\n",
    "    \n",
    "    # cutout\n",
    "    for i in range(nb_cuts):\n",
    "        y = np.random.randint(height)\n",
    "        x = np.random.randint(width)\n",
    "        length = 4 * np.random.randint(1, MAX_LENGTH_MUTIPLIER+1)\n",
    "        \n",
    "        y1 = np.clip(y-length//2, 0, height)\n",
    "        y2 = np.clip(y+length//2, 0, height)\n",
    "        x1 = np.clip(x-length//2, 0, width)\n",
    "        x2 = np.clip(x+length//2, 0, width)\n",
    "        \n",
    "        mask[y1:y2, x1:x2, :] = 0.\n",
    "    \n",
    "    img = img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU6(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"ReLU6\")\n",
    "        self.relu6 = ReLU(max_value=6, name=\"ReLU6\")\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input)\n",
    "\n",
    "\n",
    "class HardSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu6 = ReLU6()\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwish(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input * self.hard_sigmoid(input)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, ch, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.channels = ch\n",
    "        self.filters_f_g = self.channels // 8\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        print(kernel_shape_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
    "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_f')\n",
    "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_g')\n",
    "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_h')\n",
    "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_F')\n",
    "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_g')\n",
    "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4,\n",
    "                                    axes={3: input_shape[-1]})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        def hw_flatten(x):\n",
    "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
    "\n",
    "        f = K.conv2d(x,\n",
    "                     kernel=self.kernel_f,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        f = K.bias_add(f, self.bias_f)\n",
    "        g = K.conv2d(x,\n",
    "                     kernel=self.kernel_g,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        g = K.bias_add(g, self.bias_g)\n",
    "        h = K.conv2d(x,\n",
    "                     kernel=self.kernel_h,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
    "        h = K.bias_add(h, self.bias_h)\n",
    "\n",
    "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "        beta = K.softmax(s, axis=-1)  # attention map\n",
    "\n",
    "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
    "\n",
    "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 32100 images belonging to 2 classes.\n",
      "Found 37566 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ft_dir = '/home/kim3/fake_detection/task2/128x128/fine_tune'\n",
    "train_gen_aug = ImageDataGenerator(shear_range=0, \n",
    "                               zoom_range=0.2,\n",
    "                               rotation_range=0.2,\n",
    "                               width_shift_range=2., \n",
    "                               height_shift_range=2.,\n",
    "                               horizontal_flip=True,\n",
    "                               zca_whitening=False,\n",
    "                               fill_mode='nearest',\n",
    "                               rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "ft_gen = train_gen_aug.flow_from_directory(ft_dir,\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 4)\n",
      "(1, 1, 64, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0925 11:27:41.349404 139941363332864 deprecation.py:506] From /home/kim3/anaconda3/envs/fedml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 128, 16)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 2, 2, 2048)   20857032    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 2, 576)    1179648     model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 2, 2, 576)    2304        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_1 (HardSwish)        (None, 2, 2, 576)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 1, 1, 576)    5184        hard_swish_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 1, 1, 576)    2304        depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 576)          0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 1, 144)    82944       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 1, 1, 144)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 1, 576)    82944       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_2 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 2, 2, 576)    0           hard_swish_1[0][0]               \n",
      "                                                                 hard_sigmoid_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_2 (HardSwish)        (None, 2, 2, 576)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 2, 2, 128)    73728       hard_swish_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 2, 2, 128)    512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 2, 2, 576)    73728       batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 2, 2, 576)    2304        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_3 (HardSwish)        (None, 2, 2, 576)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 2, 2, 576)    14400       hard_swish_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2, 2, 576)    2304        depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_35 (SeparableC (None, 32, 32, 32)   123         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 576)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 32, 32, 32)   128         separable_conv2d_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_6 (HardSwish)        (None, 32, 32, 32)   0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 1, 1, 144)    82944       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 32, 32, 32)   1321        hard_swish_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 1, 1, 144)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_36 (SeparableC (None, 16, 16, 64)   2336        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 1, 1, 576)    82944       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_5 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_7 (HardSwish)        (None, 16, 16, 64)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 2, 2, 576)    0           hard_swish_3[0][0]               \n",
      "                                                                 hard_sigmoid_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 16, 16, 64)   5201        hard_swish_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_4 (HardSwish)        (None, 2, 2, 576)    0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_37 (SeparableC (None, 8, 8, 128)    8768        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 2, 2, 128)    73728       hard_swish_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 2, 2, 128)    512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_8 (HardSwish)        (None, 8, 8, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 128)    0           batch_normalization_40[0][0]     \n",
      "                                                                 batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 8, 8, 128)    20641       hard_swish_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 2, 2, 576)    73728       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 576)    73728       attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 2, 2, 576)    2304        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 8, 8, 576)    2304        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_5 (HardSwish)        (None, 2, 2, 576)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_9 (HardSwish)        (None, 8, 8, 576)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 576)          0           hard_swish_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 576)          0           hard_swish_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 1, 1, 576)    0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 1, 1, 1280)   738560      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_10 (HardSwish)       (None, 1, 1, 1280)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 1280)   0           hard_swish_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 1, 1, 2)      2562        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 1, 1, 2)      0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 2)         0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 2)            0           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,551,936\n",
      "Trainable params: 2,687,032\n",
      "Non-trainable params: 20,864,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model('/home/kim3/fake_detection/model/xception_94_96.h5')\n",
    "for i in range(2):\n",
    "    model_ft.layers.pop()\n",
    "im_in = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "base_model = Model(img_input, x)\n",
    "base_model.set_weights(model_ft.get_weights())\n",
    "for i in range(len(base_model.layers) - 0):\n",
    "    base_model.layers[i].trainable = False\n",
    "    \n",
    "x1 = base_model(im_in) # (12, 12, 32)\n",
    "########### Mobilenet block bneck 3x3 (32 --> 128) #################\n",
    "expand1 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(x1)\n",
    "expand1 = BatchNormalization()(expand1)\n",
    "expand1 = HardSwish()(expand1)\n",
    "dw1 = DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand1)\n",
    "dw1 = BatchNormalization()(dw1)\n",
    "se_gap1 = GlobalAveragePooling2D()(dw1)\n",
    "se_gap1 = Reshape([1, 1, -1])(se_gap1)\n",
    "se1 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap1)\n",
    "se1 = Activation('relu')(se1)\n",
    "se1 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se1)\n",
    "se1 = HardSigmoid()(se1)\n",
    "se1 = Multiply()([expand1, se1])\n",
    "project1 = HardSwish()(se1)\n",
    "project1 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "project1 = BatchNormalization()(project1)\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand2 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "expand2 = BatchNormalization()(expand2)\n",
    "expand2 = HardSwish()(expand2)\n",
    "dw2 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand2)\n",
    "dw2 = BatchNormalization()(dw2)\n",
    "se_gap2 = GlobalAveragePooling2D()(dw2)\n",
    "se_gap2 = Reshape([1, 1, -1])(se_gap2)\n",
    "se2 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap2)\n",
    "se2 = Activation('relu')(se2)\n",
    "se2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se2)\n",
    "se2 = HardSigmoid()(se2)\n",
    "se2 = Multiply()([expand2, se2])\n",
    "project2 = HardSwish()(se2)\n",
    "project2 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "project2 = BatchNormalization()(project2)\n",
    "project2 = Add()([project1, project2])\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand3 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "expand3 = BatchNormalization()(expand3)\n",
    "expand3 = HardSwish()(expand3)\n",
    "dw3 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand3)\n",
    "dw3 = BatchNormalization()(dw3)\n",
    "se_gap3 = GlobalAveragePooling2D()(dw3)\n",
    "se_gap3 = Reshape([1, 1, -1])(se_gap3)\n",
    "se3 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap3)\n",
    "se3 = Activation('relu')(se3)\n",
    "se3 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se3)\n",
    "se3 = HardSigmoid()(se3)\n",
    "se3 = Multiply()([expand3, se3])\n",
    "project3 = HardSwish()(se3)\n",
    "project3 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "project3 = BatchNormalization()(project3)\n",
    "project3 = Add()([project2, project3])\n",
    "\n",
    "expand4 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "expand4 = BatchNormalization()(expand4)\n",
    "expand4 = HardSwish()(expand4)\n",
    "dw4 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand4)\n",
    "dw4 = BatchNormalization()(dw4)\n",
    "se_gap4 = GlobalAveragePooling2D()(dw4)\n",
    "se_gap4 = Reshape([1, 1, -1])(se_gap4)\n",
    "se4 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap4)\n",
    "se4 = Activation('relu')(se4)\n",
    "se4 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se4)\n",
    "se4 = HardSigmoid()(se4)\n",
    "se4 = Multiply()([expand4, se4])\n",
    "project4 = HardSwish()(se4)\n",
    "project4 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "project4 = BatchNormalization()(project4)\n",
    "project4 = Add()([project3, project4])\n",
    "\n",
    "########## Classification ##########\n",
    "x2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "######### Image Attention Model #########\n",
    "### Block 1 ###\n",
    "x3 = SeparableConv2D(32, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(im_in)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = Attention(32)(x3)\n",
    "\n",
    "### Block 2 ###\n",
    "x4 = SeparableConv2D(64, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x3)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Attention(64)(x4)\n",
    "\n",
    "### Block 3 ###\n",
    "x5 = SeparableConv2D(128, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x4)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = Attention(128)(x5)\n",
    "\n",
    "### final stage ###\n",
    "x6 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(x5)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Activation('relu')(x6)\n",
    "x6 = GlobalAveragePooling2D()(x6)\n",
    "\n",
    "######## final addition #########\n",
    "x2 = Add()([x2, x6])\n",
    "x2 = Dense(2)(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_top = Model(inputs=im_in, outputs=x2)\n",
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "200/200 [==============================] - 57s 287ms/step - loss: 0.5069 - acc: 0.7848 - val_loss: 0.3543 - val_acc: 0.8754\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.4567 - acc: 0.8067 - val_loss: 0.3596 - val_acc: 0.8649\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.4455 - acc: 0.8114 - val_loss: 0.3205 - val_acc: 0.8799\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.4379 - acc: 0.8175 - val_loss: 0.3156 - val_acc: 0.8840\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.4298 - acc: 0.8202 - val_loss: 0.3231 - val_acc: 0.8863\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.4253 - acc: 0.8261 - val_loss: 0.3247 - val_acc: 0.8891\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.4118 - acc: 0.8281 - val_loss: 0.3087 - val_acc: 0.8885\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.4178 - acc: 0.8238 - val_loss: 0.2889 - val_acc: 0.8913\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.4072 - acc: 0.8277 - val_loss: 0.2945 - val_acc: 0.8886\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.4122 - acc: 0.8302 - val_loss: 0.2977 - val_acc: 0.8968\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.4118 - acc: 0.8259 - val_loss: 0.3177 - val_acc: 0.8826\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.4048 - acc: 0.8263 - val_loss: 0.2873 - val_acc: 0.8918\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.4061 - acc: 0.8303 - val_loss: 0.2956 - val_acc: 0.8923\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.4006 - acc: 0.8280 - val_loss: 0.3025 - val_acc: 0.8876\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.4069 - acc: 0.8294 - val_loss: 0.3184 - val_acc: 0.8758\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.4033 - acc: 0.8281 - val_loss: 0.2761 - val_acc: 0.9008\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.4011 - acc: 0.8297 - val_loss: 0.2778 - val_acc: 0.9012\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3898 - acc: 0.8331 - val_loss: 0.2969 - val_acc: 0.8980\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3867 - acc: 0.8342 - val_loss: 0.2747 - val_acc: 0.8983\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3849 - acc: 0.8344 - val_loss: 0.2670 - val_acc: 0.8998\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.3920 - acc: 0.8266 - val_loss: 0.2874 - val_acc: 0.8969\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3883 - acc: 0.8336 - val_loss: 0.2650 - val_acc: 0.9038\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.3823 - acc: 0.8336 - val_loss: 0.2560 - val_acc: 0.9046\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3791 - acc: 0.8361 - val_loss: 0.2599 - val_acc: 0.8982\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.3775 - acc: 0.8417 - val_loss: 0.2707 - val_acc: 0.9004\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.3788 - acc: 0.8367 - val_loss: 0.2559 - val_acc: 0.9029\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.3768 - acc: 0.8377 - val_loss: 0.2715 - val_acc: 0.8969\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 43s 214ms/step - loss: 0.3722 - acc: 0.8448 - val_loss: 0.2667 - val_acc: 0.8933\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.3716 - acc: 0.8389 - val_loss: 0.2528 - val_acc: 0.9027\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.3718 - acc: 0.8391 - val_loss: 0.2520 - val_acc: 0.9064\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3735 - acc: 0.8438 - val_loss: 0.2502 - val_acc: 0.9048\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3634 - acc: 0.8392 - val_loss: 0.3144 - val_acc: 0.8782\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.3577 - acc: 0.8487 - val_loss: 0.2830 - val_acc: 0.8910\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.3662 - acc: 0.8428 - val_loss: 0.2860 - val_acc: 0.8852\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.3643 - acc: 0.8484 - val_loss: 0.3080 - val_acc: 0.8965\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.3577 - acc: 0.8480 - val_loss: 0.2846 - val_acc: 0.8898\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.3504 - acc: 0.8525 - val_loss: 0.2603 - val_acc: 0.8994\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3570 - acc: 0.8458 - val_loss: 0.2768 - val_acc: 0.9004\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3484 - acc: 0.8544 - val_loss: 0.2582 - val_acc: 0.9023\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.3487 - acc: 0.8541 - val_loss: 0.2790 - val_acc: 0.8993\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3441 - acc: 0.8544 - val_loss: 0.2751 - val_acc: 0.8957\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.3410 - acc: 0.8555 - val_loss: 0.2701 - val_acc: 0.8904\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3411 - acc: 0.8547 - val_loss: 0.2598 - val_acc: 0.8972\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3555 - acc: 0.8514 - val_loss: 0.2789 - val_acc: 0.8994\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.3270 - acc: 0.8656 - val_loss: 0.2741 - val_acc: 0.9000\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3366 - acc: 0.8577 - val_loss: 0.2594 - val_acc: 0.9007\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.3251 - acc: 0.8634 - val_loss: 0.2515 - val_acc: 0.9042\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3304 - acc: 0.8597 - val_loss: 0.2599 - val_acc: 0.9003\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3209 - acc: 0.8700 - val_loss: 0.2866 - val_acc: 0.8986\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3236 - acc: 0.8700 - val_loss: 0.2974 - val_acc: 0.8861\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.3224 - acc: 0.8653 - val_loss: 0.2978 - val_acc: 0.8871\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.3095 - acc: 0.8733 - val_loss: 0.3966 - val_acc: 0.8332\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.3180 - acc: 0.8689 - val_loss: 0.2479 - val_acc: 0.9069\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.3153 - acc: 0.8692 - val_loss: 0.3235 - val_acc: 0.8701\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.3013 - acc: 0.8777 - val_loss: 0.2913 - val_acc: 0.8872\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3109 - acc: 0.8702 - val_loss: 0.2569 - val_acc: 0.9036\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.3051 - acc: 0.8742 - val_loss: 0.2625 - val_acc: 0.9033\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.3029 - acc: 0.8739 - val_loss: 0.2889 - val_acc: 0.8844\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2868 - acc: 0.8830 - val_loss: 0.2719 - val_acc: 0.8969\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2902 - acc: 0.8764 - val_loss: 0.2767 - val_acc: 0.8973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2991 - acc: 0.8789 - val_loss: 0.3011 - val_acc: 0.8880\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.3041 - acc: 0.8728 - val_loss: 0.2519 - val_acc: 0.9049\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2960 - acc: 0.8850 - val_loss: 0.2673 - val_acc: 0.9010\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2867 - acc: 0.8820 - val_loss: 0.2608 - val_acc: 0.9031\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2702 - acc: 0.8897 - val_loss: 0.2855 - val_acc: 0.8931\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2773 - acc: 0.8884 - val_loss: 0.2675 - val_acc: 0.9015\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.2630 - acc: 0.8925 - val_loss: 0.2635 - val_acc: 0.8994\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2609 - acc: 0.8958 - val_loss: 0.3003 - val_acc: 0.8951\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2666 - acc: 0.8937 - val_loss: 0.2596 - val_acc: 0.9045\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.2556 - acc: 0.9020 - val_loss: 0.3372 - val_acc: 0.8740\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.2523 - acc: 0.9044 - val_loss: 0.3038 - val_acc: 0.8901\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 43s 213ms/step - loss: 0.2475 - acc: 0.9020 - val_loss: 0.2575 - val_acc: 0.9076\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.2593 - acc: 0.9000 - val_loss: 0.3232 - val_acc: 0.8816\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.2538 - acc: 0.8967 - val_loss: 0.3029 - val_acc: 0.8919\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2437 - acc: 0.9067 - val_loss: 0.2817 - val_acc: 0.8998\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.2679 - acc: 0.8930 - val_loss: 0.2545 - val_acc: 0.9016\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.2453 - acc: 0.9044 - val_loss: 0.2626 - val_acc: 0.9025\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.2437 - acc: 0.9033 - val_loss: 0.2596 - val_acc: 0.9052\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2483 - acc: 0.9064 - val_loss: 0.2796 - val_acc: 0.9024\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.2478 - acc: 0.9048 - val_loss: 0.3046 - val_acc: 0.8849\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2465 - acc: 0.9017 - val_loss: 0.2618 - val_acc: 0.9061\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.2353 - acc: 0.9061 - val_loss: 0.3060 - val_acc: 0.8954\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2359 - acc: 0.9058 - val_loss: 0.3006 - val_acc: 0.8920\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.2312 - acc: 0.9087 - val_loss: 0.2842 - val_acc: 0.8979\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2329 - acc: 0.9077 - val_loss: 0.3117 - val_acc: 0.8906\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2358 - acc: 0.9098 - val_loss: 0.2718 - val_acc: 0.9061\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2435 - acc: 0.9038 - val_loss: 0.2768 - val_acc: 0.9002\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2121 - acc: 0.9175 - val_loss: 0.2904 - val_acc: 0.8978\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.2295 - acc: 0.9131 - val_loss: 0.2686 - val_acc: 0.9018\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.2228 - acc: 0.9164 - val_loss: 0.2927 - val_acc: 0.8944\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2330 - acc: 0.9105 - val_loss: 0.2684 - val_acc: 0.9049\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2346 - acc: 0.9097 - val_loss: 0.2673 - val_acc: 0.9040\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2227 - acc: 0.9134 - val_loss: 0.2921 - val_acc: 0.8939\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.2216 - acc: 0.9145 - val_loss: 0.2954 - val_acc: 0.9008\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.2066 - acc: 0.9189 - val_loss: 0.2999 - val_acc: 0.9009\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2112 - acc: 0.9189 - val_loss: 0.2663 - val_acc: 0.9062\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2038 - acc: 0.9208 - val_loss: 0.2998 - val_acc: 0.9017\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.2004 - acc: 0.9197 - val_loss: 0.3386 - val_acc: 0.8923\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1989 - acc: 0.9241 - val_loss: 0.3295 - val_acc: 0.8905\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.2129 - acc: 0.9191 - val_loss: 0.3215 - val_acc: 0.8880\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.2014 - acc: 0.9238 - val_loss: 0.2651 - val_acc: 0.9077\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.1964 - acc: 0.9216 - val_loss: 0.2930 - val_acc: 0.9012\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1887 - acc: 0.9319 - val_loss: 0.3030 - val_acc: 0.8979\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1792 - acc: 0.9311 - val_loss: 0.3302 - val_acc: 0.8933\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.2000 - acc: 0.9186 - val_loss: 0.2826 - val_acc: 0.9058\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1933 - acc: 0.9223 - val_loss: 0.2790 - val_acc: 0.9081\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1937 - acc: 0.9303 - val_loss: 0.3468 - val_acc: 0.8880\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1961 - acc: 0.9247 - val_loss: 0.3106 - val_acc: 0.8899\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1897 - acc: 0.9300 - val_loss: 0.2728 - val_acc: 0.9059\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1896 - acc: 0.9278 - val_loss: 0.2868 - val_acc: 0.9045\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1830 - acc: 0.9320 - val_loss: 0.2939 - val_acc: 0.9000\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.1728 - acc: 0.9331 - val_loss: 0.3038 - val_acc: 0.9032\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1679 - acc: 0.9364 - val_loss: 0.3002 - val_acc: 0.9014\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1745 - acc: 0.9339 - val_loss: 0.3149 - val_acc: 0.8953\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1754 - acc: 0.9369 - val_loss: 0.3047 - val_acc: 0.8975\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 43s 215ms/step - loss: 0.1742 - acc: 0.9353 - val_loss: 0.2983 - val_acc: 0.9033\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1727 - acc: 0.9328 - val_loss: 0.3155 - val_acc: 0.8952\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1753 - acc: 0.9309 - val_loss: 0.2930 - val_acc: 0.9038\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1619 - acc: 0.9397 - val_loss: 0.2976 - val_acc: 0.8992\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1632 - acc: 0.9391 - val_loss: 0.3091 - val_acc: 0.9034\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1568 - acc: 0.9386 - val_loss: 0.2938 - val_acc: 0.9039\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1641 - acc: 0.9375 - val_loss: 0.3044 - val_acc: 0.9011\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1642 - acc: 0.9381 - val_loss: 0.3011 - val_acc: 0.9019\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1572 - acc: 0.9423 - val_loss: 0.2867 - val_acc: 0.9079\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1622 - acc: 0.9398 - val_loss: 0.2958 - val_acc: 0.9050\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1687 - acc: 0.9350 - val_loss: 0.2914 - val_acc: 0.9046\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1752 - acc: 0.9369 - val_loss: 0.2955 - val_acc: 0.9047\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1601 - acc: 0.9409 - val_loss: 0.2887 - val_acc: 0.9058\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1601 - acc: 0.9416 - val_loss: 0.2946 - val_acc: 0.9033\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1502 - acc: 0.9461 - val_loss: 0.2882 - val_acc: 0.9077\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1585 - acc: 0.9431 - val_loss: 0.2827 - val_acc: 0.9093\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1544 - acc: 0.9422 - val_loss: 0.2989 - val_acc: 0.9045\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1475 - acc: 0.9456 - val_loss: 0.2912 - val_acc: 0.9040\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1471 - acc: 0.9480 - val_loss: 0.3042 - val_acc: 0.9023\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.1431 - acc: 0.9455 - val_loss: 0.2910 - val_acc: 0.9086\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.1446 - acc: 0.9459 - val_loss: 0.2887 - val_acc: 0.9072\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1480 - acc: 0.9453 - val_loss: 0.2902 - val_acc: 0.9058\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1438 - acc: 0.9484 - val_loss: 0.2942 - val_acc: 0.9071\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.1474 - acc: 0.9450 - val_loss: 0.3113 - val_acc: 0.9038\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1563 - acc: 0.9431 - val_loss: 0.3087 - val_acc: 0.9029\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1477 - acc: 0.9456 - val_loss: 0.2959 - val_acc: 0.9059\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1424 - acc: 0.9467 - val_loss: 0.2930 - val_acc: 0.9047\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1435 - acc: 0.9494 - val_loss: 0.3102 - val_acc: 0.9040\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1368 - acc: 0.9506 - val_loss: 0.3154 - val_acc: 0.9049\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1419 - acc: 0.9502 - val_loss: 0.2964 - val_acc: 0.9091\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1411 - acc: 0.9488 - val_loss: 0.3006 - val_acc: 0.9086\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1462 - acc: 0.9466 - val_loss: 0.2896 - val_acc: 0.9089\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1468 - acc: 0.9473 - val_loss: 0.2895 - val_acc: 0.9079\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1450 - acc: 0.9455 - val_loss: 0.2919 - val_acc: 0.9084\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1447 - acc: 0.9472 - val_loss: 0.2881 - val_acc: 0.9086\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1306 - acc: 0.9508 - val_loss: 0.2945 - val_acc: 0.9071\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1287 - acc: 0.9502 - val_loss: 0.3001 - val_acc: 0.9090\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 46s 231ms/step - loss: 0.1194 - acc: 0.9573 - val_loss: 0.3024 - val_acc: 0.9086\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1293 - acc: 0.9556 - val_loss: 0.3016 - val_acc: 0.9091\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1369 - acc: 0.9475 - val_loss: 0.3060 - val_acc: 0.9070\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1370 - acc: 0.9511 - val_loss: 0.3005 - val_acc: 0.9077\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1229 - acc: 0.9575 - val_loss: 0.3051 - val_acc: 0.9089\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1340 - acc: 0.9506 - val_loss: 0.3184 - val_acc: 0.9034\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1459 - acc: 0.9441 - val_loss: 0.3067 - val_acc: 0.9068\n",
      "Epoch 160/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1308 - acc: 0.9545 - val_loss: 0.2997 - val_acc: 0.9082\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1371 - acc: 0.9489 - val_loss: 0.2994 - val_acc: 0.9079\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1354 - acc: 0.9545 - val_loss: 0.3011 - val_acc: 0.9090\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1342 - acc: 0.9555 - val_loss: 0.3019 - val_acc: 0.9069\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1350 - acc: 0.9516 - val_loss: 0.3040 - val_acc: 0.9061\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1365 - acc: 0.9520 - val_loss: 0.2968 - val_acc: 0.9088\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1451 - acc: 0.9470 - val_loss: 0.3034 - val_acc: 0.9075\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1287 - acc: 0.9556 - val_loss: 0.2931 - val_acc: 0.9095\n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1297 - acc: 0.9527 - val_loss: 0.3015 - val_acc: 0.9075\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1349 - acc: 0.9505 - val_loss: 0.3037 - val_acc: 0.9082\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1180 - acc: 0.9583 - val_loss: 0.3076 - val_acc: 0.9068\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1244 - acc: 0.9569 - val_loss: 0.3093 - val_acc: 0.9067\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 0.1314 - acc: 0.9553 - val_loss: 0.3064 - val_acc: 0.9078\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1352 - acc: 0.9492 - val_loss: 0.3030 - val_acc: 0.9075\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1263 - acc: 0.9556 - val_loss: 0.3013 - val_acc: 0.9093\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1274 - acc: 0.9563 - val_loss: 0.3029 - val_acc: 0.9079\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1229 - acc: 0.9542 - val_loss: 0.3060 - val_acc: 0.9082\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1261 - acc: 0.9538 - val_loss: 0.3088 - val_acc: 0.9076\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1227 - acc: 0.9545 - val_loss: 0.3052 - val_acc: 0.9079\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1306 - acc: 0.9506 - val_loss: 0.3029 - val_acc: 0.9077\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1296 - acc: 0.9533 - val_loss: 0.3051 - val_acc: 0.9081\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1197 - acc: 0.9558 - val_loss: 0.3031 - val_acc: 0.9081\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1310 - acc: 0.9516 - val_loss: 0.3048 - val_acc: 0.9081\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 0.1187 - acc: 0.9575 - val_loss: 0.3052 - val_acc: 0.9075\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1288 - acc: 0.9539 - val_loss: 0.3075 - val_acc: 0.9073\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1318 - acc: 0.9527 - val_loss: 0.3093 - val_acc: 0.9069\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1201 - acc: 0.9598 - val_loss: 0.3056 - val_acc: 0.9077\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1204 - acc: 0.9584 - val_loss: 0.3076 - val_acc: 0.9073\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1302 - acc: 0.9556 - val_loss: 0.3053 - val_acc: 0.9080\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1322 - acc: 0.9534 - val_loss: 0.3071 - val_acc: 0.9074\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1202 - acc: 0.9584 - val_loss: 0.3063 - val_acc: 0.9081\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1266 - acc: 0.9527 - val_loss: 0.3050 - val_acc: 0.9086\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1290 - acc: 0.9541 - val_loss: 0.3060 - val_acc: 0.9076\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1242 - acc: 0.9556 - val_loss: 0.3062 - val_acc: 0.9081\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1199 - acc: 0.9566 - val_loss: 0.3051 - val_acc: 0.9078\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 0.1208 - acc: 0.9566 - val_loss: 0.3063 - val_acc: 0.9078\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1237 - acc: 0.9577 - val_loss: 0.3078 - val_acc: 0.9078\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1187 - acc: 0.9578 - val_loss: 0.3088 - val_acc: 0.9080\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1238 - acc: 0.9567 - val_loss: 0.3070 - val_acc: 0.9080\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1338 - acc: 0.9503 - val_loss: 0.3065 - val_acc: 0.9084\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1141 - acc: 0.9594 - val_loss: 0.3079 - val_acc: 0.9079\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1165 - acc: 0.9603 - val_loss: 0.3086 - val_acc: 0.9081\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1203 - acc: 0.9569 - val_loss: 0.3092 - val_acc: 0.9076\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1283 - acc: 0.9544 - val_loss: 0.3076 - val_acc: 0.9081\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1290 - acc: 0.9520 - val_loss: 0.3078 - val_acc: 0.9075\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1209 - acc: 0.9570 - val_loss: 0.3082 - val_acc: 0.9077\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1178 - acc: 0.9600 - val_loss: 0.3106 - val_acc: 0.9072\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1260 - acc: 0.9520 - val_loss: 0.3086 - val_acc: 0.9069\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1283 - acc: 0.9573 - val_loss: 0.3093 - val_acc: 0.9069\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1220 - acc: 0.9545 - val_loss: 0.3113 - val_acc: 0.9068\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1184 - acc: 0.9584 - val_loss: 0.3086 - val_acc: 0.9077\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1288 - acc: 0.9555 - val_loss: 0.3067 - val_acc: 0.9081\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1228 - acc: 0.9572 - val_loss: 0.3076 - val_acc: 0.9075\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1259 - acc: 0.9581 - val_loss: 0.3087 - val_acc: 0.9071\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1308 - acc: 0.9528 - val_loss: 0.3070 - val_acc: 0.9078\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1254 - acc: 0.9555 - val_loss: 0.3063 - val_acc: 0.9080\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1300 - acc: 0.9506 - val_loss: 0.3082 - val_acc: 0.9076\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1248 - acc: 0.9567 - val_loss: 0.3070 - val_acc: 0.9077\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1245 - acc: 0.9531 - val_loss: 0.3088 - val_acc: 0.9077\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1170 - acc: 0.9595 - val_loss: 0.3081 - val_acc: 0.9077\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1213 - acc: 0.9558 - val_loss: 0.3075 - val_acc: 0.9082\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1218 - acc: 0.9563 - val_loss: 0.3088 - val_acc: 0.9079\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1226 - acc: 0.9580 - val_loss: 0.3101 - val_acc: 0.9080\n",
      "Epoch 223/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1296 - acc: 0.9523 - val_loss: 0.3096 - val_acc: 0.9081\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1238 - acc: 0.9527 - val_loss: 0.3083 - val_acc: 0.9082\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1172 - acc: 0.9587 - val_loss: 0.3072 - val_acc: 0.9083\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1281 - acc: 0.9553 - val_loss: 0.3073 - val_acc: 0.9085\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1204 - acc: 0.9553 - val_loss: 0.3075 - val_acc: 0.9078\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1276 - acc: 0.9509 - val_loss: 0.3064 - val_acc: 0.9085\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1237 - acc: 0.9572 - val_loss: 0.3071 - val_acc: 0.9084\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1322 - acc: 0.9525 - val_loss: 0.3091 - val_acc: 0.9076\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1196 - acc: 0.9592 - val_loss: 0.3088 - val_acc: 0.9083\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1186 - acc: 0.9586 - val_loss: 0.3071 - val_acc: 0.9082\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1310 - acc: 0.9548 - val_loss: 0.3075 - val_acc: 0.9083\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1228 - acc: 0.9550 - val_loss: 0.3077 - val_acc: 0.9082\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1311 - acc: 0.9513 - val_loss: 0.3073 - val_acc: 0.9079\n",
      "Epoch 236/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1238 - acc: 0.9572 - val_loss: 0.3059 - val_acc: 0.9083\n",
      "Epoch 237/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1227 - acc: 0.9559 - val_loss: 0.3053 - val_acc: 0.9084\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1214 - acc: 0.9564 - val_loss: 0.3074 - val_acc: 0.9079\n",
      "Epoch 239/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1129 - acc: 0.9611 - val_loss: 0.3060 - val_acc: 0.9083\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.1131 - acc: 0.9600 - val_loss: 0.3065 - val_acc: 0.9082\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1284 - acc: 0.9539 - val_loss: 0.3053 - val_acc: 0.9086\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1183 - acc: 0.9559 - val_loss: 0.3076 - val_acc: 0.9081\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1225 - acc: 0.9556 - val_loss: 0.3089 - val_acc: 0.9074\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1233 - acc: 0.9531 - val_loss: 0.3092 - val_acc: 0.9078\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1176 - acc: 0.9559 - val_loss: 0.3059 - val_acc: 0.9085\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 46s 232ms/step - loss: 0.1111 - acc: 0.9622 - val_loss: 0.3075 - val_acc: 0.9080\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1166 - acc: 0.9594 - val_loss: 0.3099 - val_acc: 0.9081\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1352 - acc: 0.9491 - val_loss: 0.3087 - val_acc: 0.9079\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1169 - acc: 0.9592 - val_loss: 0.3108 - val_acc: 0.9075\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1205 - acc: 0.9536 - val_loss: 0.3083 - val_acc: 0.9081\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1165 - acc: 0.9591 - val_loss: 0.3094 - val_acc: 0.9074\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1251 - acc: 0.9550 - val_loss: 0.3115 - val_acc: 0.9078\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1249 - acc: 0.9572 - val_loss: 0.3100 - val_acc: 0.9080\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1260 - acc: 0.9545 - val_loss: 0.3109 - val_acc: 0.9086\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 46s 230ms/step - loss: 0.1323 - acc: 0.9516 - val_loss: 0.3105 - val_acc: 0.9075\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1159 - acc: 0.9586 - val_loss: 0.3096 - val_acc: 0.9085\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1239 - acc: 0.9539 - val_loss: 0.3088 - val_acc: 0.9080\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1258 - acc: 0.9566 - val_loss: 0.3092 - val_acc: 0.9080\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1267 - acc: 0.9561 - val_loss: 0.3093 - val_acc: 0.9082\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1196 - acc: 0.9605 - val_loss: 0.3077 - val_acc: 0.9085\n",
      "Epoch 261/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1207 - acc: 0.9578 - val_loss: 0.3101 - val_acc: 0.9082\n",
      "Epoch 262/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1243 - acc: 0.9583 - val_loss: 0.3098 - val_acc: 0.9080\n",
      "Epoch 263/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1182 - acc: 0.9595 - val_loss: 0.3099 - val_acc: 0.9082\n",
      "Epoch 264/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1252 - acc: 0.9553 - val_loss: 0.3087 - val_acc: 0.9077\n",
      "Epoch 265/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1201 - acc: 0.9577 - val_loss: 0.3112 - val_acc: 0.9077\n",
      "Epoch 266/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1251 - acc: 0.9566 - val_loss: 0.3106 - val_acc: 0.9079\n",
      "Epoch 267/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1210 - acc: 0.9600 - val_loss: 0.3107 - val_acc: 0.9074\n",
      "Epoch 268/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1208 - acc: 0.9583 - val_loss: 0.3107 - val_acc: 0.9077\n",
      "Epoch 269/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1229 - acc: 0.9558 - val_loss: 0.3107 - val_acc: 0.9075\n",
      "Epoch 270/300\n",
      "200/200 [==============================] - 45s 224ms/step - loss: 0.1199 - acc: 0.9584 - val_loss: 0.3108 - val_acc: 0.9071\n",
      "Epoch 271/300\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.1279 - acc: 0.9520 - val_loss: 0.3114 - val_acc: 0.9073\n",
      "Epoch 272/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1181 - acc: 0.9566 - val_loss: 0.3108 - val_acc: 0.9079\n",
      "Epoch 273/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1174 - acc: 0.9572 - val_loss: 0.3105 - val_acc: 0.9081\n",
      "Epoch 274/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1324 - acc: 0.9528 - val_loss: 0.3113 - val_acc: 0.9075\n",
      "Epoch 275/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1265 - acc: 0.9552 - val_loss: 0.3099 - val_acc: 0.9074\n",
      "Epoch 276/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1253 - acc: 0.9566 - val_loss: 0.3101 - val_acc: 0.9076\n",
      "Epoch 277/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1158 - acc: 0.9605 - val_loss: 0.3097 - val_acc: 0.9080\n",
      "Epoch 278/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1240 - acc: 0.9583 - val_loss: 0.3099 - val_acc: 0.9076\n",
      "Epoch 279/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1195 - acc: 0.9598 - val_loss: 0.3114 - val_acc: 0.9069\n",
      "Epoch 280/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1274 - acc: 0.9539 - val_loss: 0.3108 - val_acc: 0.9074\n",
      "Epoch 281/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1172 - acc: 0.9617 - val_loss: 0.3096 - val_acc: 0.9076\n",
      "Epoch 282/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1180 - acc: 0.9564 - val_loss: 0.3104 - val_acc: 0.9070\n",
      "Epoch 283/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1195 - acc: 0.9573 - val_loss: 0.3126 - val_acc: 0.9068\n",
      "Epoch 284/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1210 - acc: 0.9578 - val_loss: 0.3091 - val_acc: 0.9078\n",
      "Epoch 285/300\n",
      "200/200 [==============================] - 43s 216ms/step - loss: 0.1287 - acc: 0.9527 - val_loss: 0.3112 - val_acc: 0.9070\n",
      "Epoch 286/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1186 - acc: 0.9553 - val_loss: 0.3108 - val_acc: 0.9070\n",
      "Epoch 287/300\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.1296 - acc: 0.9528 - val_loss: 0.3132 - val_acc: 0.9066\n",
      "Epoch 288/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1159 - acc: 0.9584 - val_loss: 0.3118 - val_acc: 0.9069\n",
      "Epoch 289/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1173 - acc: 0.9564 - val_loss: 0.3127 - val_acc: 0.9077\n",
      "Epoch 290/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1266 - acc: 0.9548 - val_loss: 0.3110 - val_acc: 0.9072\n",
      "Epoch 291/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1304 - acc: 0.9525 - val_loss: 0.3115 - val_acc: 0.9075\n",
      "Epoch 292/300\n",
      "200/200 [==============================] - 44s 219ms/step - loss: 0.1184 - acc: 0.9583 - val_loss: 0.3114 - val_acc: 0.9073\n",
      "Epoch 293/300\n",
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1166 - acc: 0.9578 - val_loss: 0.3139 - val_acc: 0.9068\n",
      "Epoch 294/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1218 - acc: 0.9542 - val_loss: 0.3128 - val_acc: 0.9072\n",
      "Epoch 295/300\n",
      "200/200 [==============================] - 44s 218ms/step - loss: 0.1139 - acc: 0.9583 - val_loss: 0.3112 - val_acc: 0.9078\n",
      "Epoch 296/300\n",
      "200/200 [==============================] - 44s 222ms/step - loss: 0.1223 - acc: 0.9578 - val_loss: 0.3108 - val_acc: 0.9075\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 44s 221ms/step - loss: 0.1164 - acc: 0.9619 - val_loss: 0.3122 - val_acc: 0.9081\n",
      "Epoch 298/300\n",
      "200/200 [==============================] - 44s 220ms/step - loss: 0.1227 - acc: 0.9545 - val_loss: 0.3100 - val_acc: 0.9081\n",
      "Epoch 299/300\n",
      "200/200 [==============================] - 43s 217ms/step - loss: 0.1249 - acc: 0.9544 - val_loss: 0.3118 - val_acc: 0.9073\n",
      "Epoch 300/300\n",
      "200/200 [==============================] - 45s 223ms/step - loss: 0.1208 - acc: 0.9588 - val_loss: 0.3109 - val_acc: 0.9073\n"
     ]
    }
   ],
   "source": [
    "# optimizer = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model_top.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience=30), \n",
    "                 ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n",
    "output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n",
    "                                  validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1174/1174 [01:28<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_score50 = []\n",
    "output_class50 = []\n",
    "answer_class50 = []\n",
    "answer_class50_1 =[]\n",
    "\n",
    "for i in trange(len(test50_generator)):\n",
    "    output50 = model_top.predict_on_batch(test50_generator[i][0])\n",
    "    output_score50.append(output50)\n",
    "    answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "output_score50 = np.concatenate(output_score50)\n",
    "answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "output_class50 = np.argmax(output_score50, axis=1)\n",
    "answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "print(output_class50)\n",
    "print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91     18788\n",
      "           1       0.89      0.93      0.91     18778\n",
      "\n",
      "    accuracy                           0.91     37566\n",
      "   macro avg       0.91      0.91      0.91     37566\n",
      "weighted avg       0.91      0.91      0.91     37566\n",
      "\n",
      "[[16651  2137]\n",
      " [ 1345 17433]]\n",
      "AUROC: 0.963272\n",
      "0.6508347660497121\n",
      "test_acc:  0.9073098014161742\n"
     ]
    }
   ],
   "source": [
    "cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "print(report50)\n",
    "print(cm50)\n",
    "print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "print(thresh50)\n",
    "print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
