{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda, Dropout\n",
    "from keras.layers import SeparableConv2D, Add, Convolution2D, concatenate, Layer, ReLU, DepthwiseConv2D, Reshape, Multiply, InputSpec\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "from PIL import Image\n",
    "# from tqdm import trange\n",
    "import random\n",
    "from keras.applications import Xception, ResNet152\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2  # number of classes\n",
    "img_width, img_height = 64, 64  # change based on the shape/structure of your images\n",
    "batch_size = 64  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 300  # number of iteration the algorithm gets trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/a/fakedata/deepfake/train'\n",
    "validation_dir = '/mnt/a/fakedata/deepfake/val'\n",
    "test50_dir = '/mnt/a/fakedata/deepfake/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 15:19:08.663826 15500 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"squeezenet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 31, 31, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 31, 31, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 15, 15, 64)   0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 15, 15, 16)   1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 15, 16)   64          fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 15, 15, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 15, 15, 64)   1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 15, 15, 64)   9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 15, 15, 64)   0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 15, 15, 64)   0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 15, 15, 128)  0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 15, 15, 16)   2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 15, 16)   64          fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 15, 15, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 15, 15, 64)   1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 15, 15, 64)   9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 15, 15, 64)   0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 15, 15, 64)   0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 15, 15, 128)  0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 7, 7, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 7, 7, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 7, 7, 32)     128         fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 7, 7, 32)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 7, 7, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 7, 7, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 7, 7, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 7, 7, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 7, 7, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 7, 7, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 7, 7, 32)     128         fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 7, 7, 32)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 7, 7, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 7, 7, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 7, 7, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 7, 7, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 7, 7, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 3, 3, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 3, 3, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 3, 3, 48)     192         fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 3, 3, 48)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 3, 3, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 3, 3, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 3, 3, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 3, 3, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 3, 3, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 3, 3, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 3, 3, 48)     192         fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 3, 3, 48)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 3, 3, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 3, 3, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 3, 3, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 3, 3, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 3, 3, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 3, 3, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 3, 3, 64)     256         fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 3, 3, 64)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 3, 3, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 3, 3, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 3, 3, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 3, 3, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 3, 3, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 3, 3, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 3, 3, 64)     256         fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 3, 3, 64)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 3, 3, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 3, 3, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 3, 3, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 3, 3, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 3, 3, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 3, 3, 2)      1026        fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 3, 3, 2)      0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2)            0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 2)            0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 724,802\n",
      "Trainable params: 724,162\n",
      "Non-trainable params: 640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "# Modular function for Fire Node\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "#     left = BatchNormalization()(left)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "#     right = BatchNormalization()(right)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Original SqueezeNet from paper.\n",
    "\n",
    "\n",
    "\n",
    "img_input = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "# x = BatchNormalization()(x)\n",
    "x = Activation('relu', name='relu_conv1')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "\n",
    "# x_dp = Dropout(0.5, name='drop9')(x)\n",
    "x_conv = Convolution2D(nb_classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "x_relu = Activation('relu', name='relu_conv10')(x_conv)\n",
    "x_gap = GlobalAveragePooling2D()(x_relu)\n",
    "x_sm = Activation('softmax', name='loss')(x_gap)\n",
    "\n",
    "model = Model(img_input, x_sm, name='squeezenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_list = [EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "#                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "# history = model.fit_generator(train_generator,\n",
    "#                             steps_per_epoch=200,\n",
    "#                             epochs=100,\n",
    "#                             validation_data=validation_generator,\n",
    "#                             validation_steps=len(validation_generator),\n",
    "#                             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/www/fake_detection/model/deepfake_squeezenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('/home/www/fake_detection/model/deepfake_squeezenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.predict_generator(test50_generator, steps=len(test50_generator), verbose=1)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# print(test50_generator.class_indices)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_score50 = []\n",
    "# output_class50 = []\n",
    "# answer_class50 = []\n",
    "# answer_class50_1 =[]\n",
    "\n",
    "# for i in range(len(test50_generator)):\n",
    "#     output50 = model.predict_on_batch(test50_generator[i][0])\n",
    "#     output_score50.append(output50)\n",
    "#     answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "# output_score50 = np.concatenate(output_score50)\n",
    "# answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "# output_class50 = np.argmax(output_score50, axis=1)\n",
    "# answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "# print(output_class50)\n",
    "# print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "# report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "# recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "# fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "# fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "# eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "# thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "# print(report50)\n",
    "# print(cm50)\n",
    "# print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "# print(thresh50)\n",
    "# print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img):\n",
    "    \"\"\"\n",
    "    # Function: RandomCrop (ZeroPadded (4, 4)) + random occulusion image\n",
    "    # Arguments:\n",
    "        img: image\n",
    "    # Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = bgr(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]\n",
    "    MAX_CUTS = 3 # chance to get more cuts\n",
    "    MAX_LENGTH_MUTIPLIER = 5 # chance to get larger cuts\n",
    "    # 16 for cifar10, 8 for cifar100\n",
    "    \n",
    "    # Zero-padded (4, 4)\n",
    "#     img = np.pad(img, ((4,4),(4,4),(0,0)), mode='constant', constant_values=(0))\n",
    "    \n",
    "#     # random-crop 64x64\n",
    "#     dy, dx = height, width\n",
    "#     x = np.random.randint(0, width - dx + 1)\n",
    "#     y = np.random.randint(0, height - dy + 1)\n",
    "#     img = img[y:(y+dy), x:(x+dx)]\n",
    "    \n",
    "#     mean norm\n",
    "#     mean = img.mean(keepdims=True)\n",
    "#     img -= mean\n",
    "\n",
    "    img *= 1./255\n",
    "    \n",
    "    mask = np.ones((height, width, channels), dtype=np.float32)\n",
    "    nb_cuts = np.random.randint(0, MAX_CUTS + 1)\n",
    "    \n",
    "    # cutout\n",
    "    for i in range(nb_cuts):\n",
    "        y = np.random.randint(height)\n",
    "        x = np.random.randint(width)\n",
    "        length = 4 * np.random.randint(1, MAX_LENGTH_MUTIPLIER+1)\n",
    "        \n",
    "        y1 = np.clip(y-length//2, 0, height)\n",
    "        y2 = np.clip(y+length//2, 0, height)\n",
    "        x1 = np.clip(x-length//2, 0, width)\n",
    "        x2 = np.clip(x+length//2, 0, width)\n",
    "        \n",
    "        mask[y1:y2, x1:x2, :] = 0.\n",
    "    \n",
    "    img = img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU6(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"ReLU6\")\n",
    "        self.relu6 = ReLU(max_value=6, name=\"ReLU6\")\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input)\n",
    "\n",
    "\n",
    "class HardSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu6 = ReLU6()\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwish(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input * self.hard_sigmoid(input)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, ch, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.channels = ch\n",
    "        self.filters_f_g = self.channels // 8\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        print(kernel_shape_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
    "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_f')\n",
    "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_g')\n",
    "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_h')\n",
    "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_F')\n",
    "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_g')\n",
    "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4,\n",
    "                                    axes={3: input_shape[-1]})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        def hw_flatten(x):\n",
    "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
    "\n",
    "        f = K.conv2d(x,\n",
    "                     kernel=self.kernel_f,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        f = K.bias_add(f, self.bias_f)\n",
    "        g = K.conv2d(x,\n",
    "                     kernel=self.kernel_g,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        g = K.bias_add(g, self.bias_g)\n",
    "        h = K.conv2d(x,\n",
    "                     kernel=self.kernel_h,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
    "        h = K.bias_add(h, self.bias_h)\n",
    "\n",
    "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "        beta = K.softmax(s, axis=-1)  # attention map\n",
    "\n",
    "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
    "\n",
    "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ft_dir = '/mnt/a/fakedata/deepfake/finetune'\n",
    "train_gen_aug = ImageDataGenerator(shear_range=0, \n",
    "                               zoom_range=0,\n",
    "                               rotation_range=0.2,\n",
    "                               width_shift_range=2., \n",
    "                               height_shift_range=2.,\n",
    "                               horizontal_flip=True,\n",
    "                               zca_whitening=False,\n",
    "                               fill_mode='nearest',\n",
    "                               preprocessing_function=cutout)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "ft_gen = train_gen_aug.flow_from_directory(ft_dir,\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1007 15:19:25.848282 15500 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 4)\n",
      "(1, 1, 64, 8)\n",
      "(1, 1, 128, 16)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 3, 3, 512)    723776      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 3, 3, 576)    294912      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 3, 3, 576)    2304        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_1 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 2, 2, 576)    5184        hard_swish_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 2, 2, 576)    2304        depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 576)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 1, 144)    82944       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 1, 144)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 1, 576)    82944       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_2 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 3, 3, 576)    0           hard_swish_1[0][0]               \n",
      "                                                                 hard_sigmoid_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_2 (HardSwish)        (None, 3, 3, 576)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 128)    73728       hard_swish_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 3, 3, 128)    512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 576)    73728       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 3, 3, 576)    2304        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_3 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 3, 3, 576)    14400       hard_swish_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 3, 3, 576)    2304        depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 576)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 1, 144)    82944       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 1, 144)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 1, 1, 576)    82944       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_5 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 3, 3, 576)    0           hard_swish_3[0][0]               \n",
      "                                                                 hard_sigmoid_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_4 (HardSwish)        (None, 3, 3, 576)    0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 3, 128)    73728       hard_swish_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 3, 3, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 3, 3, 128)    0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 3, 3, 576)    73728       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 3, 3, 576)    2304        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_5 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 3, 3, 576)    14400       hard_swish_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 3, 3, 576)    2304        depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 576)          0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 144)    82944       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1, 1, 144)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 576)    82944       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_8 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 3, 3, 576)    0           hard_swish_5[0][0]               \n",
      "                                                                 hard_sigmoid_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_6 (HardSwish)        (None, 3, 3, 576)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 3, 3, 128)    73728       hard_swish_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 3, 3, 128)    512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 3, 3, 128)    0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 3, 3, 576)    73728       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 3, 3, 576)    2304        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_7 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_4 (DepthwiseCo (None, 3, 3, 576)    14400       hard_swish_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 3, 3, 576)    2304        depthwise_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 32)   123         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 576)          0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 32)   128         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 1, 1, 144)    82944       reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 32, 32, 32)   1321        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 1, 144)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 64)   2336        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 1, 576)    82944       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_11 (HardSigmoid)   (None, 1, 1, 576)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 3, 3, 576)    0           hard_swish_7[0][0]               \n",
      "                                                                 hard_sigmoid_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 16, 16, 64)   5201        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_8 (HardSwish)        (None, 3, 3, 576)    0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 128)    8768        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 3, 3, 128)    73728       hard_swish_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 3, 3, 128)    512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 3, 3, 128)    0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 8, 8, 128)    20641       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 3, 3, 576)    73728       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 576)    73728       attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 3, 3, 576)    2304        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 576)    2304        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_9 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 576)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 576)          0           hard_swish_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 576)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 576)          0           global_average_pooling2d_6[0][0] \n",
      "                                                                 global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1154        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 2)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,459,704\n",
      "Trainable params: 2,446,072\n",
      "Non-trainable params: 13,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model('/home/www/fake_detection/model/deepfake_squeezenet.h5')\n",
    "for i in range(5):\n",
    "    model_ft.layers.pop()\n",
    "im_in = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "base_model = Model(img_input, x)\n",
    "base_model.set_weights(model_ft.get_weights())\n",
    "# for i in range(len(base_model.layers) - 0):\n",
    "#     base_model.layers[i].trainable = False\n",
    "    \n",
    "x1 = base_model(im_in) # (12, 12, 32)\n",
    "########### Mobilenet block bneck 3x3 (32 --> 128) #################\n",
    "expand1 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(x1)\n",
    "expand1 = BatchNormalization()(expand1)\n",
    "expand1 = HardSwish()(expand1)\n",
    "dw1 = DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand1)\n",
    "dw1 = BatchNormalization()(dw1)\n",
    "se_gap1 = GlobalAveragePooling2D()(dw1)\n",
    "se_gap1 = Reshape([1, 1, -1])(se_gap1)\n",
    "se1 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap1)\n",
    "se1 = Activation('relu')(se1)\n",
    "se1 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se1)\n",
    "se1 = HardSigmoid()(se1)\n",
    "se1 = Multiply()([expand1, se1])\n",
    "project1 = HardSwish()(se1)\n",
    "project1 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "project1 = BatchNormalization()(project1)\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand2 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "expand2 = BatchNormalization()(expand2)\n",
    "expand2 = HardSwish()(expand2)\n",
    "dw2 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand2)\n",
    "dw2 = BatchNormalization()(dw2)\n",
    "se_gap2 = GlobalAveragePooling2D()(dw2)\n",
    "se_gap2 = Reshape([1, 1, -1])(se_gap2)\n",
    "se2 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap2)\n",
    "se2 = Activation('relu')(se2)\n",
    "se2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se2)\n",
    "se2 = HardSigmoid()(se2)\n",
    "se2 = Multiply()([expand2, se2])\n",
    "project2 = HardSwish()(se2)\n",
    "project2 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "project2 = BatchNormalization()(project2)\n",
    "project2 = Add()([project1, project2])\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand3 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "expand3 = BatchNormalization()(expand3)\n",
    "expand3 = HardSwish()(expand3)\n",
    "dw3 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand3)\n",
    "dw3 = BatchNormalization()(dw3)\n",
    "se_gap3 = GlobalAveragePooling2D()(dw3)\n",
    "se_gap3 = Reshape([1, 1, -1])(se_gap3)\n",
    "se3 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap3)\n",
    "se3 = Activation('relu')(se3)\n",
    "se3 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se3)\n",
    "se3 = HardSigmoid()(se3)\n",
    "se3 = Multiply()([expand3, se3])\n",
    "project3 = HardSwish()(se3)\n",
    "project3 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "project3 = BatchNormalization()(project3)\n",
    "project3 = Add()([project2, project3])\n",
    "\n",
    "\n",
    "expand4 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "expand4 = BatchNormalization()(expand4)\n",
    "expand4 = HardSwish()(expand4)\n",
    "dw4 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand4)\n",
    "dw4 = BatchNormalization()(dw4)\n",
    "se_gap4 = GlobalAveragePooling2D()(dw4)\n",
    "se_gap4 = Reshape([1, 1, -1])(se_gap4)\n",
    "se4 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap4)\n",
    "se4 = Activation('relu')(se4)\n",
    "se4 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se4)\n",
    "se4 = HardSigmoid()(se4)\n",
    "se4 = Multiply()([expand4, se4])\n",
    "project4 = HardSwish()(se4)\n",
    "project4 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "project4 = BatchNormalization()(project4)\n",
    "project4 = Add()([project3, project4])\n",
    "\n",
    "\n",
    "########## Classification ##########\n",
    "x2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "\n",
    "######### Image Attention Model #########\n",
    "### Block 1 ###\n",
    "x3 = SeparableConv2D(32, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(im_in)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = Attention(32)(x3)\n",
    "\n",
    "### Block 2 ###\n",
    "x4 = SeparableConv2D(64, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x3)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Attention(64)(x4)\n",
    "\n",
    "### Block 3 ###\n",
    "x5 = SeparableConv2D(128, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x4)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = Attention(128)(x5)\n",
    "\n",
    "### final stage ###\n",
    "x6 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(x5)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Activation('relu')(x6)\n",
    "x6 = GlobalAveragePooling2D()(x6)\n",
    "# x6 = Reshape([1, 1, -1])(x6)\n",
    "\n",
    "######## final addition #########\n",
    "\n",
    "x2 = Add()([x2, x6])\n",
    "x2 = Dense(2, kernel_regularizer=l2(1e-5))(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_top = Model(inputs=im_in, outputs=x2)\n",
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "200/200 [==============================] - 848s 4s/step - loss: 0.7336 - acc: 0.5748 - val_loss: 1.7832 - val_acc: 0.5981\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 927s 5s/step - loss: 0.6453 - acc: 0.6634 - val_loss: 2.1032 - val_acc: 0.5603\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 933s 5s/step - loss: 0.5406 - acc: 0.7518 - val_loss: 0.3214 - val_acc: 0.5651\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 919s 5s/step - loss: 0.4622 - acc: 0.7988 - val_loss: 4.5145 - val_acc: 0.6424\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 928s 5s/step - loss: 0.4044 - acc: 0.8253 - val_loss: 5.2729 - val_acc: 0.7074\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 921s 5s/step - loss: 0.3482 - acc: 0.8609 - val_loss: 3.1937 - val_acc: 0.7482\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 938s 5s/step - loss: 0.3181 - acc: 0.8717 - val_loss: 0.0444 - val_acc: 0.6567\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 974s 5s/step - loss: 0.2855 - acc: 0.8865 - val_loss: 0.6765 - val_acc: 0.8262\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 922s 5s/step - loss: 0.2792 - acc: 0.8914 - val_loss: 0.4435 - val_acc: 0.8290\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 895s 4s/step - loss: 0.2394 - acc: 0.9100 - val_loss: 0.6918 - val_acc: 0.8668\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 895s 4s/step - loss: 0.2283 - acc: 0.9102 - val_loss: 0.2374 - val_acc: 0.8845\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 889s 4s/step - loss: 0.2034 - acc: 0.9237 - val_loss: 1.8498 - val_acc: 0.8217\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 887s 4s/step - loss: 0.2034 - acc: 0.9228 - val_loss: 1.9934 - val_acc: 0.7468\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 873s 4s/step - loss: 0.1873 - acc: 0.9305 - val_loss: 1.5102 - val_acc: 0.8595\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 905s 5s/step - loss: 0.1646 - acc: 0.9398 - val_loss: 1.0888 - val_acc: 0.8556\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 881s 4s/step - loss: 0.1501 - acc: 0.9449 - val_loss: 1.7234 - val_acc: 0.8130\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 890s 4s/step - loss: 0.1599 - acc: 0.9413 - val_loss: 0.0240 - val_acc: 0.7773\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 930s 5s/step - loss: 0.1509 - acc: 0.9426 - val_loss: 0.4546 - val_acc: 0.9124\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 921s 5s/step - loss: 0.1368 - acc: 0.9471 - val_loss: 0.2109 - val_acc: 0.9058\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 900s 4s/step - loss: 0.1267 - acc: 0.9543 - val_loss: 0.0576 - val_acc: 0.8808\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 892s 4s/step - loss: 0.1175 - acc: 0.9593 - val_loss: 0.4046 - val_acc: 0.8947\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 902s 5s/step - loss: 0.1187 - acc: 0.9564 - val_loss: 0.2029 - val_acc: 0.8909\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 917s 5s/step - loss: 0.1235 - acc: 0.9552 - val_loss: 1.0032 - val_acc: 0.8694\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 906s 5s/step - loss: 0.1119 - acc: 0.9601 - val_loss: 0.9996 - val_acc: 0.8840\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 901s 5s/step - loss: 0.1067 - acc: 0.9606 - val_loss: 0.9998 - val_acc: 0.8768\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 889s 4s/step - loss: 0.0996 - acc: 0.9637 - val_loss: 0.1479 - val_acc: 0.8683\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 899s 4s/step - loss: 0.0990 - acc: 0.9629 - val_loss: 0.3930 - val_acc: 0.8522\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 884s 4s/step - loss: 0.1119 - acc: 0.9603 - val_loss: 2.3916 - val_acc: 0.8117\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 867s 4s/step - loss: 0.0910 - acc: 0.9679 - val_loss: 1.3406 - val_acc: 0.8514\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 857s 4s/step - loss: 0.1039 - acc: 0.9624 - val_loss: 1.2360 - val_acc: 0.8436\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 892s 4s/step - loss: 0.0963 - acc: 0.9660 - val_loss: 1.3041 - val_acc: 0.8734\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 867s 4s/step - loss: 0.0873 - acc: 0.9681 - val_loss: 0.0069 - val_acc: 0.8911\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 865s 4s/step - loss: 0.0760 - acc: 0.9718 - val_loss: 0.1047 - val_acc: 0.9229\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 856s 4s/step - loss: 0.0848 - acc: 0.9685 - val_loss: 0.0539 - val_acc: 0.8511\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 875s 4s/step - loss: 0.0868 - acc: 0.9684 - val_loss: 0.8803 - val_acc: 0.8797\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 850s 4s/step - loss: 0.0808 - acc: 0.9725 - val_loss: 0.4050 - val_acc: 0.8860\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 844s 4s/step - loss: 0.0792 - acc: 0.9730 - val_loss: 0.4314 - val_acc: 0.9036\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 840s 4s/step - loss: 0.0801 - acc: 0.9735 - val_loss: 0.4688 - val_acc: 0.8768\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 851s 4s/step - loss: 0.0627 - acc: 0.9780 - val_loss: 0.0861 - val_acc: 0.9260\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 838s 4s/step - loss: 0.0562 - acc: 0.9807 - val_loss: 0.0442 - val_acc: 0.9390\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 849s 4s/step - loss: 0.0610 - acc: 0.9787 - val_loss: 0.6795 - val_acc: 0.8595\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 835s 4s/step - loss: 0.0603 - acc: 0.9781 - val_loss: 0.3483 - val_acc: 0.9353\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 843s 4s/step - loss: 0.0639 - acc: 0.9767 - val_loss: 0.3780 - val_acc: 0.9306\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 855s 4s/step - loss: 0.0785 - acc: 0.9720 - val_loss: 0.6872 - val_acc: 0.8879\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 832s 4s/step - loss: 0.0647 - acc: 0.9779 - val_loss: 0.1921 - val_acc: 0.9291\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 841s 4s/step - loss: 0.0508 - acc: 0.9809 - val_loss: 0.1670 - val_acc: 0.9183\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 834s 4s/step - loss: 0.0533 - acc: 0.9802 - val_loss: 0.1674 - val_acc: 0.9370\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 855s 4s/step - loss: 0.0469 - acc: 0.9827 - val_loss: 0.0208 - val_acc: 0.9376\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 828s 4s/step - loss: 0.0477 - acc: 0.9818 - val_loss: 0.2751 - val_acc: 0.9375\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 841s 4s/step - loss: 0.0457 - acc: 0.9844 - val_loss: 0.0066 - val_acc: 0.9458\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 831s 4s/step - loss: 0.0434 - acc: 0.9839 - val_loss: 0.3432 - val_acc: 0.9358\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 860s 4s/step - loss: 0.0480 - acc: 0.9838 - val_loss: 0.0565 - val_acc: 0.9038\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 822s 4s/step - loss: 0.0407 - acc: 0.9855 - val_loss: 0.0183 - val_acc: 0.9437\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 836s 4s/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.1062 - val_acc: 0.9363\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 826s 4s/step - loss: 0.0448 - acc: 0.9850 - val_loss: 0.5754 - val_acc: 0.9346\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 855s 4s/step - loss: 0.0388 - acc: 0.9860 - val_loss: 0.2270 - val_acc: 0.9236\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 826s 4s/step - loss: 0.0451 - acc: 0.9829 - val_loss: 0.1297 - val_acc: 0.9365\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 829s 4s/step - loss: 0.0363 - acc: 0.9864 - val_loss: 0.3225 - val_acc: 0.9259\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 831s 4s/step - loss: 0.0409 - acc: 0.9850 - val_loss: 0.1431 - val_acc: 0.9320\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 847s 4s/step - loss: 0.0404 - acc: 0.9846 - val_loss: 0.1577 - val_acc: 0.9369\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 855s 4s/step - loss: 0.0404 - acc: 0.9837 - val_loss: 1.0221 - val_acc: 0.8961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300\n",
      "200/200 [==============================] - 825s 4s/step - loss: 0.0372 - acc: 0.9858 - val_loss: 0.0389 - val_acc: 0.9322\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 825s 4s/step - loss: 0.0426 - acc: 0.9848 - val_loss: 0.0106 - val_acc: 0.9373\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 826s 4s/step - loss: 0.0431 - acc: 0.9852 - val_loss: 0.0408 - val_acc: 0.9362\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 848s 4s/step - loss: 0.0301 - acc: 0.9888 - val_loss: 0.0077 - val_acc: 0.9429\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 830s 4s/step - loss: 0.0338 - acc: 0.9874 - val_loss: 0.2505 - val_acc: 0.9381\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 834s 4s/step - loss: 0.0301 - acc: 0.9895 - val_loss: 0.1264 - val_acc: 0.9462\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 827s 4s/step - loss: 0.0364 - acc: 0.9874 - val_loss: 0.1113 - val_acc: 0.9326\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 848s 4s/step - loss: 0.0314 - acc: 0.9871 - val_loss: 0.4205 - val_acc: 0.9172\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 825s 4s/step - loss: 0.0299 - acc: 0.9886 - val_loss: 0.3117 - val_acc: 0.9402\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 837s 4s/step - loss: 0.0293 - acc: 0.9891 - val_loss: 0.4797 - val_acc: 0.8956\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 836s 4s/step - loss: 0.0370 - acc: 0.9871 - val_loss: 0.7103 - val_acc: 0.8906\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 855s 4s/step - loss: 0.0311 - acc: 0.9876 - val_loss: 0.0381 - val_acc: 0.9532\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 882s 4s/step - loss: 0.0290 - acc: 0.9895 - val_loss: 0.1378 - val_acc: 0.9522\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 875s 4s/step - loss: 0.0279 - acc: 0.9894 - val_loss: 0.5679 - val_acc: 0.9422\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 867s 4s/step - loss: 0.0280 - acc: 0.9893 - val_loss: 0.0054 - val_acc: 0.9516\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 877s 4s/step - loss: 0.0274 - acc: 0.9888 - val_loss: 0.0345 - val_acc: 0.9503\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 882s 4s/step - loss: 0.0306 - acc: 0.9881 - val_loss: 1.0804 - val_acc: 0.9330\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 887s 4s/step - loss: 0.0288 - acc: 0.9888 - val_loss: 0.3659 - val_acc: 0.9346\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 911s 5s/step - loss: 0.0277 - acc: 0.9892 - val_loss: 0.0612 - val_acc: 0.9501\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 924s 5s/step - loss: 0.0303 - acc: 0.9883 - val_loss: 0.4486 - val_acc: 0.9458\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 909s 5s/step - loss: 0.0306 - acc: 0.9872 - val_loss: 0.3199 - val_acc: 0.9423\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 904s 5s/step - loss: 0.0266 - acc: 0.9895 - val_loss: 0.3778 - val_acc: 0.9383\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 898s 4s/step - loss: 0.0261 - acc: 0.9896 - val_loss: 0.2106 - val_acc: 0.9529\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 902s 5s/step - loss: 0.0258 - acc: 0.9897 - val_loss: 0.1303 - val_acc: 0.9560\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 894s 4s/step - loss: 0.0233 - acc: 0.9915 - val_loss: 0.0489 - val_acc: 0.9535\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 879s 4s/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.1682 - val_acc: 0.9497\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 881s 4s/step - loss: 0.0212 - acc: 0.9918 - val_loss: 0.1544 - val_acc: 0.9417\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 910s 5s/step - loss: 0.0237 - acc: 0.9913 - val_loss: 0.1418 - val_acc: 0.9533\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 903s 5s/step - loss: 0.0237 - acc: 0.9910 - val_loss: 0.1495 - val_acc: 0.9493\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 909s 5s/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.2001 - val_acc: 0.9471\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 915s 5s/step - loss: 0.0245 - acc: 0.9901 - val_loss: 0.2207 - val_acc: 0.9506\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 921s 5s/step - loss: 0.0261 - acc: 0.9896 - val_loss: 0.2232 - val_acc: 0.9473\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 942s 5s/step - loss: 0.0233 - acc: 0.9906 - val_loss: 0.2348 - val_acc: 0.9425\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 915s 5s/step - loss: 0.0212 - acc: 0.9918 - val_loss: 0.2525 - val_acc: 0.9513\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 922s 5s/step - loss: 0.0214 - acc: 0.9924 - val_loss: 0.2529 - val_acc: 0.9542\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 938s 5s/step - loss: 0.0214 - acc: 0.9910 - val_loss: 0.6684 - val_acc: 0.9387\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 928s 5s/step - loss: 0.0203 - acc: 0.9919 - val_loss: 0.1816 - val_acc: 0.9560\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 914s 5s/step - loss: 0.0214 - acc: 0.9914 - val_loss: 0.3092 - val_acc: 0.9380\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 938s 5s/step - loss: 0.0208 - acc: 0.9918 - val_loss: 0.2258 - val_acc: 0.9523\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 956s 5s/step - loss: 0.0249 - acc: 0.9904 - val_loss: 0.0785 - val_acc: 0.9538\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 908s 5s/step - loss: 0.0189 - acc: 0.9925 - val_loss: 0.2437 - val_acc: 0.9553\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 923s 5s/step - loss: 0.0220 - acc: 0.9913 - val_loss: 0.3290 - val_acc: 0.9533\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 906s 5s/step - loss: 0.0201 - acc: 0.9922 - val_loss: 0.1274 - val_acc: 0.9492\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 938s 5s/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.2239 - val_acc: 0.9503\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 918s 5s/step - loss: 0.0220 - acc: 0.9913 - val_loss: 0.0329 - val_acc: 0.9573\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 924s 5s/step - loss: 0.0205 - acc: 0.9903 - val_loss: 0.5313 - val_acc: 0.9504\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 929s 5s/step - loss: 0.0220 - acc: 0.9907 - val_loss: 0.2608 - val_acc: 0.9382\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 935s 5s/step - loss: 0.0202 - acc: 0.9909 - val_loss: 0.1978 - val_acc: 0.9521\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 883s 4s/step - loss: 0.0190 - acc: 0.9916 - val_loss: 0.3087 - val_acc: 0.9445\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 886s 4s/step - loss: 0.0204 - acc: 0.9916 - val_loss: 0.1261 - val_acc: 0.9342\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 871s 4s/step - loss: 0.0221 - acc: 0.9910 - val_loss: 0.1284 - val_acc: 0.9536\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 884s 4s/step - loss: 0.0215 - acc: 0.9920 - val_loss: 0.1758 - val_acc: 0.9531\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 876s 4s/step - loss: 0.0187 - acc: 0.9919 - val_loss: 0.2087 - val_acc: 0.9527\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 851s 4s/step - loss: 0.0191 - acc: 0.9925 - val_loss: 0.1958 - val_acc: 0.9486\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 849s 4s/step - loss: 0.0185 - acc: 0.9918 - val_loss: 0.1306 - val_acc: 0.9546\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 857s 4s/step - loss: 0.0171 - acc: 0.9927 - val_loss: 0.1746 - val_acc: 0.9502\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 857s 4s/step - loss: 0.0205 - acc: 0.9910 - val_loss: 0.2374 - val_acc: 0.9519\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 857s 4s/step - loss: 0.0213 - acc: 0.9915 - val_loss: 0.2829 - val_acc: 0.9553\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 866s 4s/step - loss: 0.0192 - acc: 0.9913 - val_loss: 0.1635 - val_acc: 0.9550\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 856s 4s/step - loss: 0.0178 - acc: 0.9928 - val_loss: 0.2466 - val_acc: 0.9513\n",
      "Epoch 122/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 864s 4s/step - loss: 0.0166 - acc: 0.9929 - val_loss: 0.1241 - val_acc: 0.9533\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 844s 4s/step - loss: 0.0167 - acc: 0.9936 - val_loss: 0.0958 - val_acc: 0.9572\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 847s 4s/step - loss: 0.0191 - acc: 0.9922 - val_loss: 0.0989 - val_acc: 0.9548\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 851s 4s/step - loss: 0.0175 - acc: 0.9917 - val_loss: 0.1520 - val_acc: 0.9566\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 849s 4s/step - loss: 0.0193 - acc: 0.9924 - val_loss: 0.2240 - val_acc: 0.9414\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 827s 4s/step - loss: 0.0206 - acc: 0.9913 - val_loss: 0.2074 - val_acc: 0.9509\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 830s 4s/step - loss: 0.0170 - acc: 0.9920 - val_loss: 0.1924 - val_acc: 0.9549\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 841s 4s/step - loss: 0.0221 - acc: 0.9909 - val_loss: 0.1415 - val_acc: 0.9496\n",
      "Epoch 130/300\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0205 - acc: 0.9919"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4ba000940af6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n\u001b[0;32m      6\u001b[0m output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n\u001b[1;32m----> 7\u001b[1;33m                                   validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m                     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(uid, i)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \"\"\"\n\u001b[1;32m--> 406\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[0;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    228\u001b[0m                            \u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                            \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                            interpolation=self.interpolation)\n\u001b[0m\u001b[0;32m    231\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[1;31m# Pillow images should be closed after `load_img`,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2777\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2779\u001b[1;33m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2781\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument"
     ]
    }
   ],
   "source": [
    "# optimizer = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model_top.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience=30), \n",
    "                 ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n",
    "output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n",
    "                                  validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "output_score50 = []\n",
    "output_class50 = []\n",
    "answer_class50 = []\n",
    "answer_class50_1 =[]\n",
    "\n",
    "for i in range(len(test50_generator)):\n",
    "    output50 = model_top.predict_on_batch(test50_generator[i][0])\n",
    "    output_score50.append(output50)\n",
    "    answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "output_score50 = np.concatenate(output_score50)\n",
    "answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "output_class50 = np.argmax(output_score50, axis=1)\n",
    "answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "print(output_class50)\n",
    "print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     10000\n",
      "           1       0.98      0.92      0.95     10000\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "[[9824  176]\n",
      " [ 760 9240]]\n",
      "AUROC: 0.991162\n",
      "0.06274624913931034\n",
      "test_acc:  0.9532\n"
     ]
    }
   ],
   "source": [
    "cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "print(report50)\n",
    "print(cm50)\n",
    "print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "print(thresh50)\n",
    "print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save('/home/www/fake_detection/model/deepfake_squeezenet_ft.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
