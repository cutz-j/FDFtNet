{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda, Dropout\n",
    "from keras.layers import SeparableConv2D, Add, Convolution2D, concatenate, Layer, ReLU, DepthwiseConv2D, Reshape, Multiply, InputSpec\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "from keras.applications import Xception, VGG16\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2  # number of classes\n",
    "img_width, img_height = 64, 64  # change based on the shape/structure of your images\n",
    "batch_size = 128  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 300  # number of iteration the algorithm gets trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/www/fake_detection/task2/128x128/train'\n",
    "validation_dir = '/home/www/fake_detection/task2/128x128/validation'\n",
    "test50_dir = '/home/www/fake_detection/task2/128x128/test_50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"squeezenet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 31, 31, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv1 (Activation)         (None, 31, 31, 64)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 15, 15, 64)   0           relu_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fire2/squeeze1x1 (Conv2D)       (None, 15, 15, 16)   1040        pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_squeeze1x1 (Activati (None, 15, 15, 16)   0           fire2/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand1x1 (Conv2D)        (None, 15, 15, 64)   1088        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/expand3x3 (Conv2D)        (None, 15, 15, 64)   9280        fire2/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand1x1 (Activatio (None, 15, 15, 64)   0           fire2/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/relu_expand3x3 (Activatio (None, 15, 15, 64)   0           fire2/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire2/concat (Concatenate)      (None, 15, 15, 128)  0           fire2/relu_expand1x1[0][0]       \n",
      "                                                                 fire2/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire3/squeeze1x1 (Conv2D)       (None, 15, 15, 16)   2064        fire2/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_squeeze1x1 (Activati (None, 15, 15, 16)   0           fire3/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand1x1 (Conv2D)        (None, 15, 15, 64)   1088        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/expand3x3 (Conv2D)        (None, 15, 15, 64)   9280        fire3/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand1x1 (Activatio (None, 15, 15, 64)   0           fire3/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/relu_expand3x3 (Activatio (None, 15, 15, 64)   0           fire3/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire3/concat (Concatenate)      (None, 15, 15, 128)  0           fire3/relu_expand1x1[0][0]       \n",
      "                                                                 fire3/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 7, 7, 128)    0           fire3/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire4/squeeze1x1 (Conv2D)       (None, 7, 7, 32)     4128        pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_squeeze1x1 (Activati (None, 7, 7, 32)     0           fire4/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand1x1 (Conv2D)        (None, 7, 7, 128)    4224        fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/expand3x3 (Conv2D)        (None, 7, 7, 128)    36992       fire4/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand1x1 (Activatio (None, 7, 7, 128)    0           fire4/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/relu_expand3x3 (Activatio (None, 7, 7, 128)    0           fire4/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire4/concat (Concatenate)      (None, 7, 7, 256)    0           fire4/relu_expand1x1[0][0]       \n",
      "                                                                 fire4/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire5/squeeze1x1 (Conv2D)       (None, 7, 7, 32)     8224        fire4/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_squeeze1x1 (Activati (None, 7, 7, 32)     0           fire5/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand1x1 (Conv2D)        (None, 7, 7, 128)    4224        fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/expand3x3 (Conv2D)        (None, 7, 7, 128)    36992       fire5/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand1x1 (Activatio (None, 7, 7, 128)    0           fire5/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/relu_expand3x3 (Activatio (None, 7, 7, 128)    0           fire5/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire5/concat (Concatenate)      (None, 7, 7, 256)    0           fire5/relu_expand1x1[0][0]       \n",
      "                                                                 fire5/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 3, 3, 256)    0           fire5/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire6/squeeze1x1 (Conv2D)       (None, 3, 3, 48)     12336       pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_squeeze1x1 (Activati (None, 3, 3, 48)     0           fire6/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand1x1 (Conv2D)        (None, 3, 3, 192)    9408        fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/expand3x3 (Conv2D)        (None, 3, 3, 192)    83136       fire6/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand1x1 (Activatio (None, 3, 3, 192)    0           fire6/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/relu_expand3x3 (Activatio (None, 3, 3, 192)    0           fire6/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire6/concat (Concatenate)      (None, 3, 3, 384)    0           fire6/relu_expand1x1[0][0]       \n",
      "                                                                 fire6/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire7/squeeze1x1 (Conv2D)       (None, 3, 3, 48)     18480       fire6/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_squeeze1x1 (Activati (None, 3, 3, 48)     0           fire7/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand1x1 (Conv2D)        (None, 3, 3, 192)    9408        fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/expand3x3 (Conv2D)        (None, 3, 3, 192)    83136       fire7/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand1x1 (Activatio (None, 3, 3, 192)    0           fire7/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/relu_expand3x3 (Activatio (None, 3, 3, 192)    0           fire7/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire7/concat (Concatenate)      (None, 3, 3, 384)    0           fire7/relu_expand1x1[0][0]       \n",
      "                                                                 fire7/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire8/squeeze1x1 (Conv2D)       (None, 3, 3, 64)     24640       fire7/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_squeeze1x1 (Activati (None, 3, 3, 64)     0           fire8/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand1x1 (Conv2D)        (None, 3, 3, 256)    16640       fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/expand3x3 (Conv2D)        (None, 3, 3, 256)    147712      fire8/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand1x1 (Activatio (None, 3, 3, 256)    0           fire8/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/relu_expand3x3 (Activatio (None, 3, 3, 256)    0           fire8/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire8/concat (Concatenate)      (None, 3, 3, 512)    0           fire8/relu_expand1x1[0][0]       \n",
      "                                                                 fire8/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "fire9/squeeze1x1 (Conv2D)       (None, 3, 3, 64)     32832       fire8/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_squeeze1x1 (Activati (None, 3, 3, 64)     0           fire9/squeeze1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand1x1 (Conv2D)        (None, 3, 3, 256)    16640       fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/expand3x3 (Conv2D)        (None, 3, 3, 256)    147712      fire9/relu_squeeze1x1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand1x1 (Activatio (None, 3, 3, 256)    0           fire9/expand1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/relu_expand3x3 (Activatio (None, 3, 3, 256)    0           fire9/expand3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "fire9/concat (Concatenate)      (None, 3, 3, 512)    0           fire9/relu_expand1x1[0][0]       \n",
      "                                                                 fire9/relu_expand3x3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv10 (Conv2D)                 (None, 3, 3, 2)      1026        fire9/concat[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu_conv10 (Activation)        (None, 3, 3, 2)      0           conv10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2)            0           relu_conv10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "loss (Activation)               (None, 2)            0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 723,522\n",
      "Trainable params: 723,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sq1x1 = \"squeeze1x1\"\n",
    "exp1x1 = \"expand1x1\"\n",
    "exp3x3 = \"expand3x3\"\n",
    "relu = \"relu_\"\n",
    "# Modular function for Fire Node\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "    \n",
    "    x = Convolution2D(squeeze, (1, 1), padding='valid', name=s_id + sq1x1)(x)\n",
    "    x = Activation('relu', name=s_id + relu + sq1x1)(x)\n",
    "\n",
    "    left = Convolution2D(expand, (1, 1), padding='valid', name=s_id + exp1x1)(x)\n",
    "    left = Activation('relu', name=s_id + relu + exp1x1)(left)\n",
    "\n",
    "    right = Convolution2D(expand, (3, 3), padding='same', name=s_id + exp3x3)(x)\n",
    "    right = Activation('relu', name=s_id + relu + exp3x3)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis, name=s_id + 'concat')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Original SqueezeNet from paper.\n",
    "\n",
    "\n",
    "\n",
    "img_input = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n",
    "x = Activation('relu', name='relu_conv1')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "\n",
    "x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "\n",
    "# x_dp = Dropout(0.5, name='drop9')(x)\n",
    "x_conv = Convolution2D(nb_classes, (1, 1), padding='valid', name='conv10')(x)\n",
    "x_relu = Activation('relu', name='relu_conv10')(x_conv)\n",
    "x_gap = GlobalAveragePooling2D()(x_relu)\n",
    "x_sm = Activation('softmax', name='loss')(x_gap)\n",
    "\n",
    "model = Model(img_input, x_sm, name='squeezenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128404 images belonging to 2 classes.\n",
      "Found 32100 images belonging to 2 classes.\n",
      "Found 37566 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range=0.0, \n",
    "                                   shear_range=0.0,\n",
    "                                   zoom_range=0.0,\n",
    "                                   width_shift_range=0.0,\n",
    "                                   height_shift_range=0.0,\n",
    "                                   horizontal_flip=False,\n",
    "                                   rescale=1./255,)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_list = [EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "#                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "# history = model.fit_generator(train_generator,\n",
    "#                             steps_per_epoch=200,\n",
    "#                             epochs=100,\n",
    "#                             validation_data=validation_generator,\n",
    "#                             validation_steps=len(validation_generator),\n",
    "#                             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/www/fake_detection/model/celeba_squeezenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = load_model('/home/www/fake_detection/model/celeba_squeezenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 28s 95ms/step\n",
      "{'0_real': 0, '1_fake': 1}\n",
      "[[0.500 0.500]\n",
      " [0.500 0.500]\n",
      " [0.500 0.500]\n",
      " ...\n",
      " [0.500 0.500]\n",
      " [0.500 0.500]\n",
      " [0.500 0.500]]\n"
     ]
    }
   ],
   "source": [
    "# output = model.predict_generator(test50_generator, steps=len(test50_generator), verbose=1)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# print(test50_generator.class_indices)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [00:52<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output_score50 = []\n",
    "# output_class50 = []\n",
    "# answer_class50 = []\n",
    "# answer_class50_1 =[]\n",
    "\n",
    "# for i in trange(len(test50_generator)):\n",
    "#     output50 = model.predict_on_batch(test50_generator[i][0])\n",
    "#     output_score50.append(output50)\n",
    "#     answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "# output_score50 = np.concatenate(output_score50)\n",
    "# answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "# output_class50 = np.argmax(output_score50, axis=1)\n",
    "# answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "# print(output_class50)\n",
    "# print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     18788\n",
      "           1       0.00      0.00      0.00     18778\n",
      "\n",
      "    accuracy                           0.50     37566\n",
      "   macro avg       0.25      0.50      0.33     37566\n",
      "weighted avg       0.25      0.50      0.33     37566\n",
      "\n",
      "[[18788     0]\n",
      " [18778     0]]\n",
      "AUROC: 0.500000\n",
      "1.0\n",
      "test_acc:  0.5001330990789544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/www/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "# report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "# recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "# fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "# fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "# eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "# thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "# print(report50)\n",
    "# print(cm50)\n",
    "# print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "# print(thresh50)\n",
    "# print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img):\n",
    "    \"\"\"\n",
    "    # Function: RandomCrop (ZeroPadded (4, 4)) + random occulusion image\n",
    "    # Arguments:\n",
    "        img: image\n",
    "    # Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]\n",
    "    MAX_CUTS = 3 # chance to get more cuts\n",
    "    MAX_LENGTH_MUTIPLIER = 5 # chance to get larger cuts\n",
    "    # 16 for cifar10, 8 for cifar100\n",
    "    \n",
    "    # Zero-padded (4, 4)\n",
    "#     img = np.pad(img, ((4,4),(4,4),(0,0)), mode='constant', constant_values=(0))\n",
    "    \n",
    "#     # random-crop 64x64\n",
    "#     dy, dx = height, width\n",
    "#     x = np.random.randint(0, width - dx + 1)\n",
    "#     y = np.random.randint(0, height - dy + 1)\n",
    "#     img = img[y:(y+dy), x:(x+dx)]\n",
    "    \n",
    "#     mean norm\n",
    "#     mean = img.mean(keepdims=True)\n",
    "#     img -= mean\n",
    "\n",
    "    img *= 1./255\n",
    "    \n",
    "    mask = np.ones((height, width, channels), dtype=np.float32)\n",
    "    nb_cuts = np.random.randint(0, MAX_CUTS + 1)\n",
    "    \n",
    "    # cutout\n",
    "    for i in range(nb_cuts):\n",
    "        y = np.random.randint(height)\n",
    "        x = np.random.randint(width)\n",
    "        length = 4 * np.random.randint(1, MAX_LENGTH_MUTIPLIER+1)\n",
    "        \n",
    "        y1 = np.clip(y-length//2, 0, height)\n",
    "        y2 = np.clip(y+length//2, 0, height)\n",
    "        x1 = np.clip(x-length//2, 0, width)\n",
    "        x2 = np.clip(x+length//2, 0, width)\n",
    "        \n",
    "        mask[y1:y2, x1:x2, :] = 0.\n",
    "    \n",
    "    img = img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU6(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"ReLU6\")\n",
    "        self.relu6 = ReLU(max_value=6, name=\"ReLU6\")\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input)\n",
    "\n",
    "\n",
    "class HardSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu6 = ReLU6()\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwish(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input * self.hard_sigmoid(input)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, ch, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.channels = ch\n",
    "        self.filters_f_g = self.channels // 8\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        print(kernel_shape_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
    "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_f')\n",
    "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_g')\n",
    "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_h')\n",
    "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_F')\n",
    "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_g')\n",
    "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4,\n",
    "                                    axes={3: input_shape[-1]})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        def hw_flatten(x):\n",
    "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
    "\n",
    "        f = K.conv2d(x,\n",
    "                     kernel=self.kernel_f,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        f = K.bias_add(f, self.bias_f)\n",
    "        g = K.conv2d(x,\n",
    "                     kernel=self.kernel_g,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        g = K.bias_add(g, self.bias_g)\n",
    "        h = K.conv2d(x,\n",
    "                     kernel=self.kernel_h,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
    "        h = K.bias_add(h, self.bias_h)\n",
    "\n",
    "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "        beta = K.softmax(s, axis=-1)  # attention map\n",
    "\n",
    "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
    "\n",
    "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 32100 images belonging to 2 classes.\n",
      "Found 37566 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ft_dir = '/home/www/fake_detection/task2/128x128/fine_tune'\n",
    "train_gen_aug = ImageDataGenerator(shear_range=0, \n",
    "                               zoom_range=0.2,\n",
    "                               rotation_range=0.2,\n",
    "                               width_shift_range=2., \n",
    "                               height_shift_range=2.,\n",
    "                               horizontal_flip=True,\n",
    "                               zca_whitening=False,\n",
    "                               fill_mode='nearest',\n",
    "                               preprocessing_function=cutout)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "ft_gen = train_gen_aug.flow_from_directory(ft_dir,\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 32, 4)\n",
      "(1, 1, 64, 8)\n",
      "(1, 1, 128, 16)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, 3, 3, 512)    722496      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 3, 3, 576)    294912      model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 3, 3, 576)    2304        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_1 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 2, 2, 576)    5184        hard_swish_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2, 2, 576)    2304        depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 576)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 1, 144)    82944       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 1, 144)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 1, 576)    82944       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_2 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 3, 3, 576)    0           hard_swish_1[0][0]               \n",
      "                                                                 hard_sigmoid_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_2 (HardSwish)        (None, 3, 3, 576)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 128)    73728       hard_swish_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 3, 3, 128)    512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 576)    73728       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3, 3, 576)    2304        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_3 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_2 (DepthwiseCo (None, 3, 3, 576)    14400       hard_swish_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 3, 3, 576)    2304        depthwise_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 576)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 1, 144)    82944       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 1, 144)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 1, 1, 576)    82944       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_5 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 3, 3, 576)    0           hard_swish_3[0][0]               \n",
      "                                                                 hard_sigmoid_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_4 (HardSwish)        (None, 3, 3, 576)    0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 3, 128)    73728       hard_swish_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 3, 3, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 3, 3, 128)    0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 3, 3, 576)    73728       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 3, 3, 576)    2304        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_5 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_3 (DepthwiseCo (None, 3, 3, 576)    14400       hard_swish_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 3, 3, 576)    2304        depthwise_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 32)   123         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 576)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_8 (HardSwish)        (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 144)    82944       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 32, 32, 32)   1321        hard_swish_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1, 1, 144)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 64)   2336        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 576)    82944       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_8 (HardSigmoid)    (None, 1, 1, 576)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_9 (HardSwish)        (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 3, 3, 576)    0           hard_swish_5[0][0]               \n",
      "                                                                 hard_sigmoid_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 16, 16, 64)   5201        hard_swish_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_6 (HardSwish)        (None, 3, 3, 576)    0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 128)    8768        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 3, 3, 128)    73728       hard_swish_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 3, 3, 128)    512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_10 (HardSwish)       (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 3, 3, 128)    0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 8, 8, 128)    20641       hard_swish_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 3, 3, 576)    73728       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 576)    73728       attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 3, 3, 576)    2304        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 576)    2304        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_7 (HardSwish)        (None, 3, 3, 576)    0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_11 (HardSwish)       (None, 8, 8, 576)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 576)          0           hard_swish_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 576)          0           hard_swish_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1, 1, 576)    0           global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1, 576)    0           reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 1, 1020)   588540      add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_12 (HardSwish)       (None, 1, 1, 1020)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 1020)   0           hard_swish_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 1, 1, 2)      2042        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 1, 2)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 2)         0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 2)            0           lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,714,988\n",
      "Trainable params: 1,982,060\n",
      "Non-trainable params: 732,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model('/home/www/fake_detection/model/celeba_squeezenet.h5')\n",
    "for i in range(4):\n",
    "    model_ft.layers.pop()\n",
    "im_in = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "base_model = Model(img_input, x)\n",
    "base_model.set_weights(model_ft.get_weights())\n",
    "for i in range(len(base_model.layers) - 0):\n",
    "    base_model.layers[i].trainable = False\n",
    "    \n",
    "x1 = base_model(im_in) # (12, 12, 32)\n",
    "########### Mobilenet block bneck 3x3 (32 --> 128) #################\n",
    "expand1 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(x1)\n",
    "expand1 = BatchNormalization()(expand1)\n",
    "expand1 = HardSwish()(expand1)\n",
    "dw1 = DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand1)\n",
    "dw1 = BatchNormalization()(dw1)\n",
    "se_gap1 = GlobalAveragePooling2D()(dw1)\n",
    "se_gap1 = Reshape([1, 1, -1])(se_gap1)\n",
    "se1 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap1)\n",
    "se1 = Activation('relu')(se1)\n",
    "se1 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se1)\n",
    "se1 = HardSigmoid()(se1)\n",
    "se1 = Multiply()([expand1, se1])\n",
    "project1 = HardSwish()(se1)\n",
    "project1 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "project1 = BatchNormalization()(project1)\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand2 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "expand2 = BatchNormalization()(expand2)\n",
    "expand2 = HardSwish()(expand2)\n",
    "dw2 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand2)\n",
    "dw2 = BatchNormalization()(dw2)\n",
    "se_gap2 = GlobalAveragePooling2D()(dw2)\n",
    "se_gap2 = Reshape([1, 1, -1])(se_gap2)\n",
    "se2 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap2)\n",
    "se2 = Activation('relu')(se2)\n",
    "se2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se2)\n",
    "se2 = HardSigmoid()(se2)\n",
    "se2 = Multiply()([expand2, se2])\n",
    "project2 = HardSwish()(se2)\n",
    "project2 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "project2 = BatchNormalization()(project2)\n",
    "project2 = Add()([project1, project2])\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand3 = Conv2D(576, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "expand3 = BatchNormalization()(expand3)\n",
    "expand3 = HardSwish()(expand3)\n",
    "dw3 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand3)\n",
    "dw3 = BatchNormalization()(dw3)\n",
    "se_gap3 = GlobalAveragePooling2D()(dw3)\n",
    "se_gap3 = Reshape([1, 1, -1])(se_gap3)\n",
    "se3 = Conv2D(144, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap3)\n",
    "se3 = Activation('relu')(se3)\n",
    "se3 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se3)\n",
    "se3 = HardSigmoid()(se3)\n",
    "se3 = Multiply()([expand3, se3])\n",
    "project3 = HardSwish()(se3)\n",
    "project3 = Conv2D(128, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "project3 = BatchNormalization()(project3)\n",
    "project3 = Add()([project2, project3])\n",
    "\n",
    "\n",
    "########## Classification ##########\n",
    "x2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "x2 = Reshape([1, 1, -1])(x2)\n",
    "\n",
    "\n",
    "######### Image Attention Model #########\n",
    "### Block 1 ###\n",
    "x3 = SeparableConv2D(32, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(im_in)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = HardSwish()(x3)\n",
    "x3 = Attention(32)(x3)\n",
    "\n",
    "### Block 2 ###\n",
    "x4 = SeparableConv2D(64, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x3)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = HardSwish()(x4)\n",
    "x4 = Attention(64)(x4)\n",
    "\n",
    "### Block 3 ###\n",
    "x5 = SeparableConv2D(128, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x4)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = HardSwish()(x5)\n",
    "x5 = Attention(128)(x5)\n",
    "\n",
    "### final stage ###\n",
    "x6 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(x5)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = HardSwish()(x6)\n",
    "x6 = GlobalAveragePooling2D()(x6)\n",
    "x6 = Reshape([1, 1, -1])(x6)\n",
    "\n",
    "######## final addition #########\n",
    "\n",
    "x2 = Add()([x2, x6])\n",
    "x2 = Conv2D(1020, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5))(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "x2 = Conv2D(2, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5))(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "x2 = Lambda(lambda x: K.squeeze(x, 1))(x2)\n",
    "x2 = Lambda(lambda x: K.squeeze(x, 1))(x2)\n",
    "\n",
    "model_top = Model(inputs=im_in, outputs=x2)\n",
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.7025 - acc: 0.6183 - val_loss: 0.9523 - val_acc: 0.5000\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.5348 - acc: 0.7343 - val_loss: 0.9417 - val_acc: 0.5105\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 83s 416ms/step - loss: 0.4535 - acc: 0.7805 - val_loss: 5.1508 - val_acc: 0.5438\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.3932 - acc: 0.8190 - val_loss: 3.1349 - val_acc: 0.6293\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 96s 479ms/step - loss: 0.3602 - acc: 0.8379 - val_loss: 0.0110 - val_acc: 0.5000\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 100s 501ms/step - loss: 0.3090 - acc: 0.8660 - val_loss: 4.7389 - val_acc: 0.5270\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.2887 - acc: 0.8773 - val_loss: 1.2142 - val_acc: 0.7450\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 69s 347ms/step - loss: 0.2554 - acc: 0.8969 - val_loss: 0.2884 - val_acc: 0.8775\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.2324 - acc: 0.9095 - val_loss: 1.5510 - val_acc: 0.7241\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.1997 - acc: 0.9227 - val_loss: 6.7256 - val_acc: 0.6209\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 74s 370ms/step - loss: 0.1900 - acc: 0.9273 - val_loss: 0.8695 - val_acc: 0.8076\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 72s 358ms/step - loss: 0.1743 - acc: 0.9339 - val_loss: 1.2957 - val_acc: 0.7543\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.1556 - acc: 0.9446 - val_loss: 16.1292 - val_acc: 0.5000\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 99s 497ms/step - loss: 0.1455 - acc: 0.9498 - val_loss: 2.8643 - val_acc: 0.7054\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 98s 490ms/step - loss: 0.1484 - acc: 0.9463 - val_loss: 3.0476 - val_acc: 0.6450\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 99s 496ms/step - loss: 0.1223 - acc: 0.9577 - val_loss: 1.7488 - val_acc: 0.7500\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 97s 484ms/step - loss: 0.1170 - acc: 0.9604 - val_loss: 0.8354 - val_acc: 0.7964\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 102s 509ms/step - loss: 0.1185 - acc: 0.9602 - val_loss: 0.8613 - val_acc: 0.8098\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 98s 488ms/step - loss: 0.1053 - acc: 0.9652 - val_loss: 0.5682 - val_acc: 0.8549\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 103s 513ms/step - loss: 0.1029 - acc: 0.9670 - val_loss: 2.6405 - val_acc: 0.6682\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 100s 500ms/step - loss: 0.1022 - acc: 0.9664 - val_loss: 0.6793 - val_acc: 0.8566\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 100s 501ms/step - loss: 0.0909 - acc: 0.9704 - val_loss: 1.0875 - val_acc: 0.8252\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 101s 506ms/step - loss: 0.0834 - acc: 0.9749 - val_loss: 16.1299 - val_acc: 0.5000\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 94s 470ms/step - loss: 0.0852 - acc: 0.9739 - val_loss: 0.3132 - val_acc: 0.7953\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 87s 437ms/step - loss: 0.0776 - acc: 0.9758 - val_loss: 0.3611 - val_acc: 0.7962\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 0.0828 - acc: 0.9736 - val_loss: 8.0856 - val_acc: 0.6088\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 94s 468ms/step - loss: 0.0847 - acc: 0.9733 - val_loss: 1.2144 - val_acc: 0.8144\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 102s 508ms/step - loss: 0.0719 - acc: 0.9785 - val_loss: 0.0118 - val_acc: 0.5432\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 101s 503ms/step - loss: 0.0715 - acc: 0.9786 - val_loss: 7.5506 - val_acc: 0.5999\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 101s 507ms/step - loss: 0.0668 - acc: 0.9801 - val_loss: 0.0474 - val_acc: 0.9058\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 98s 490ms/step - loss: 0.0650 - acc: 0.9819 - val_loss: 0.3405 - val_acc: 0.8903\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 88s 441ms/step - loss: 0.0631 - acc: 0.9817 - val_loss: 0.3466 - val_acc: 0.9104\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.0669 - acc: 0.9800 - val_loss: 3.1706 - val_acc: 0.7020\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.0674 - acc: 0.9808 - val_loss: 0.3266 - val_acc: 0.8975\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 75s 374ms/step - loss: 0.0659 - acc: 0.9819 - val_loss: 0.5238 - val_acc: 0.8500\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.0631 - acc: 0.9810 - val_loss: 1.3835 - val_acc: 0.8076\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 73s 364ms/step - loss: 0.0589 - acc: 0.9842 - val_loss: 7.9129 - val_acc: 0.6344\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 76s 378ms/step - loss: 0.0681 - acc: 0.9799 - val_loss: 0.7400 - val_acc: 0.8821\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 74s 371ms/step - loss: 0.0551 - acc: 0.9842 - val_loss: 0.8898 - val_acc: 0.8530\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 76s 379ms/step - loss: 0.0521 - acc: 0.9858 - val_loss: 0.8930 - val_acc: 0.8552\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 76s 379ms/step - loss: 0.0554 - acc: 0.9849 - val_loss: 0.4293 - val_acc: 0.8841\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.0569 - acc: 0.9832 - val_loss: 1.5559 - val_acc: 0.8001\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 80s 400ms/step - loss: 0.0578 - acc: 0.9839 - val_loss: 0.4668 - val_acc: 0.8961\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 0.0505 - acc: 0.9866 - val_loss: 0.2370 - val_acc: 0.8909\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 92s 458ms/step - loss: 0.0474 - acc: 0.9878 - val_loss: 1.4318 - val_acc: 0.8174\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.0540 - acc: 0.9860 - val_loss: 1.3669 - val_acc: 0.8299\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 70s 349ms/step - loss: 0.0498 - acc: 0.9867 - val_loss: 1.3571 - val_acc: 0.7968\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 71s 353ms/step - loss: 0.0509 - acc: 0.9875 - val_loss: 3.6538 - val_acc: 0.5896\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 75s 377ms/step - loss: 0.0436 - acc: 0.9890 - val_loss: 16.1307 - val_acc: 0.5000\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.0511 - acc: 0.9864 - val_loss: 3.0648 - val_acc: 0.7287\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 94s 468ms/step - loss: 0.0479 - acc: 0.9874 - val_loss: 0.5957 - val_acc: 0.8947\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 96s 481ms/step - loss: 0.0489 - acc: 0.9873 - val_loss: 2.1401 - val_acc: 0.8044\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 99s 495ms/step - loss: 0.0542 - acc: 0.9867 - val_loss: 16.1316 - val_acc: 0.5000\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 97s 483ms/step - loss: 0.0535 - acc: 0.9861 - val_loss: 4.6629 - val_acc: 0.6661\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 100s 499ms/step - loss: 0.0337 - acc: 0.9930 - val_loss: 3.3460 - val_acc: 0.7266\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 98s 492ms/step - loss: 0.0338 - acc: 0.9928 - val_loss: 10.5665 - val_acc: 0.5583\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 96s 481ms/step - loss: 0.0321 - acc: 0.9937 - val_loss: 1.8418 - val_acc: 0.8000\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 90s 451ms/step - loss: 0.0338 - acc: 0.9934 - val_loss: 3.9116 - val_acc: 0.6301\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 93s 465ms/step - loss: 0.0313 - acc: 0.9937 - val_loss: 2.0788 - val_acc: 0.7847\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.0359 - acc: 0.9921 - val_loss: 16.1306 - val_acc: 0.5000\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 79s 393ms/step - loss: 0.0379 - acc: 0.9912 - val_loss: 2.0784 - val_acc: 0.8246\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.0323 - acc: 0.9935 - val_loss: 1.6274 - val_acc: 0.8202\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.0326 - acc: 0.9934 - val_loss: 5.7599 - val_acc: 0.6352\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.0316 - acc: 0.9932 - val_loss: 3.3653 - val_acc: 0.7338\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.0306 - acc: 0.9930 - val_loss: 1.2762 - val_acc: 0.8441\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0283 - acc: 0.9946 - val_loss: 7.4948 - val_acc: 0.5426\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.0274 - acc: 0.9947 - val_loss: 1.5535 - val_acc: 0.8160\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.0269 - acc: 0.9950 - val_loss: 0.2862 - val_acc: 0.8977\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.0262 - acc: 0.9949 - val_loss: 0.4403 - val_acc: 0.9138\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.0267 - acc: 0.9950 - val_loss: 0.3150 - val_acc: 0.9201\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0272 - acc: 0.9945 - val_loss: 0.6492 - val_acc: 0.8408\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 80s 400ms/step - loss: 0.0269 - acc: 0.9951 - val_loss: 9.6755 - val_acc: 0.5161\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.0270 - acc: 0.9951 - val_loss: 8.1818 - val_acc: 0.6137\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.0316 - acc: 0.9931 - val_loss: 0.6404 - val_acc: 0.8780\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0247 - acc: 0.9952 - val_loss: 0.6120 - val_acc: 0.8877\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0222 - acc: 0.9965 - val_loss: 0.6336 - val_acc: 0.8544\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0215 - acc: 0.9968 - val_loss: 0.0485 - val_acc: 0.9080\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.0212 - acc: 0.9963 - val_loss: 16.1292 - val_acc: 0.5000\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.0204 - acc: 0.9969 - val_loss: 0.7516 - val_acc: 0.8322\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.0219 - acc: 0.9965 - val_loss: 1.4104 - val_acc: 0.8468\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.0215 - acc: 0.9965 - val_loss: 2.9981 - val_acc: 0.7408\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 80s 400ms/step - loss: 0.0229 - acc: 0.9960 - val_loss: 3.3808 - val_acc: 0.7264\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.0217 - acc: 0.9962 - val_loss: 5.1231 - val_acc: 0.7383\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0207 - acc: 0.9966 - val_loss: 1.2738 - val_acc: 0.8575\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.0200 - acc: 0.9969 - val_loss: 2.1045 - val_acc: 0.7742\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 80s 401ms/step - loss: 0.0176 - acc: 0.9977 - val_loss: 3.4223 - val_acc: 0.7259\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.0203 - acc: 0.9967 - val_loss: 0.7450 - val_acc: 0.8920\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.0198 - acc: 0.9970 - val_loss: 0.6159 - val_acc: 0.8894\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.0179 - acc: 0.9974 - val_loss: 0.9475 - val_acc: 0.8769\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.0178 - acc: 0.9976 - val_loss: 16.1284 - val_acc: 0.5000\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.0184 - acc: 0.9972 - val_loss: 0.7646 - val_acc: 0.8800\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.0182 - acc: 0.9973 - val_loss: 0.6044 - val_acc: 0.9082\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.0194 - acc: 0.9976 - val_loss: 16.1283 - val_acc: 0.5000\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.0181 - acc: 0.9977 - val_loss: 0.5934 - val_acc: 0.8909\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0166 - acc: 0.9980 - val_loss: 0.6904 - val_acc: 0.8879\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.0158 - acc: 0.9983 - val_loss: 1.6503 - val_acc: 0.8150\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.0163 - acc: 0.9978 - val_loss: 0.7528 - val_acc: 0.9067\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0168 - acc: 0.9978 - val_loss: 1.9133 - val_acc: 0.7893\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 0.0176 - acc: 0.9972 - val_loss: 0.8316 - val_acc: 0.9021\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0162 - acc: 0.9979 - val_loss: 2.1476 - val_acc: 0.8053\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.0163 - acc: 0.9978 - val_loss: 15.8705 - val_acc: 0.5000\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.0163 - acc: 0.9979 - val_loss: 0.5891 - val_acc: 0.8959\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0149 - acc: 0.9982 - val_loss: 10.1689 - val_acc: 0.5092\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 78s 392ms/step - loss: 0.0151 - acc: 0.9986 - val_loss: 0.5237 - val_acc: 0.8955\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 80s 398ms/step - loss: 0.0154 - acc: 0.9982 - val_loss: 0.8305 - val_acc: 0.8960\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 79s 396ms/step - loss: 0.0148 - acc: 0.9985 - val_loss: 16.1278 - val_acc: 0.5000\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0157 - acc: 0.9982 - val_loss: 0.3622 - val_acc: 0.8900\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.0145 - acc: 0.9986 - val_loss: 1.0633 - val_acc: 0.8488\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.0162 - acc: 0.9982 - val_loss: 1.1130 - val_acc: 0.8574\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 81s 403ms/step - loss: 0.0160 - acc: 0.9979 - val_loss: 1.5541 - val_acc: 0.7875\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.0153 - acc: 0.9983 - val_loss: 16.1276 - val_acc: 0.5000\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0148 - acc: 0.9983 - val_loss: 0.7971 - val_acc: 0.8905\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 80s 400ms/step - loss: 0.0144 - acc: 0.9984 - val_loss: 0.1110 - val_acc: 0.9152\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.0152 - acc: 0.9980 - val_loss: 0.9641 - val_acc: 0.8783\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 72s 360ms/step - loss: 0.0140 - acc: 0.9984 - val_loss: 0.1711 - val_acc: 0.9100\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 71s 355ms/step - loss: 0.0152 - acc: 0.9980 - val_loss: 0.4250 - val_acc: 0.8890\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 0.0138 - acc: 0.9985 - val_loss: 0.8766 - val_acc: 0.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 0.0150 - acc: 0.9982 - val_loss: 2.0180 - val_acc: 0.7844\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 68s 341ms/step - loss: 0.0131 - acc: 0.9990 - val_loss: 0.4235 - val_acc: 0.9019\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 70s 351ms/step - loss: 0.0138 - acc: 0.9987 - val_loss: 2.7249 - val_acc: 0.7396\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 68s 339ms/step - loss: 0.0134 - acc: 0.9988 - val_loss: 0.9185 - val_acc: 0.8846\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 71s 353ms/step - loss: 0.0143 - acc: 0.9985 - val_loss: 1.5241 - val_acc: 0.8359\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0150 - acc: 0.9982 - val_loss: 0.4256 - val_acc: 0.9116\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 68s 341ms/step - loss: 0.0150 - acc: 0.9982 - val_loss: 3.3411 - val_acc: 0.6925\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 0.0150 - acc: 0.9982 - val_loss: 0.9542 - val_acc: 0.8640\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 70s 348ms/step - loss: 0.0140 - acc: 0.9986 - val_loss: 0.6223 - val_acc: 0.8886\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 0.0135 - acc: 0.9986 - val_loss: 2.0027 - val_acc: 0.7869\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 68s 340ms/step - loss: 0.0126 - acc: 0.9988 - val_loss: 0.3306 - val_acc: 0.9084\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 0.0135 - acc: 0.9987 - val_loss: 0.3735 - val_acc: 0.9030\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 70s 348ms/step - loss: 0.0134 - acc: 0.9986 - val_loss: 0.3727 - val_acc: 0.9283\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 72s 359ms/step - loss: 0.0139 - acc: 0.9988 - val_loss: 0.2956 - val_acc: 0.9287\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 0.0139 - acc: 0.9984 - val_loss: 1.3063 - val_acc: 0.8425\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0126 - acc: 0.9990 - val_loss: 1.2029 - val_acc: 0.8512\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 0.0129 - acc: 0.9989 - val_loss: 0.7658 - val_acc: 0.8874\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0137 - acc: 0.9986 - val_loss: 0.3014 - val_acc: 0.9228\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 68s 340ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.4216 - val_acc: 0.9047\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0125 - acc: 0.9990 - val_loss: 0.2398 - val_acc: 0.9162\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.0128 - acc: 0.9990 - val_loss: 0.4022 - val_acc: 0.9230\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 0.0134 - acc: 0.9986 - val_loss: 0.6456 - val_acc: 0.8801\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 70s 352ms/step - loss: 0.0134 - acc: 0.9986 - val_loss: 0.5464 - val_acc: 0.9063\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 67s 337ms/step - loss: 0.0125 - acc: 0.9987 - val_loss: 1.1720 - val_acc: 0.8754\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 68s 340ms/step - loss: 0.0117 - acc: 0.9991 - val_loss: 0.4024 - val_acc: 0.8974\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0124 - acc: 0.9989 - val_loss: 0.4554 - val_acc: 0.9072\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 69s 343ms/step - loss: 0.0132 - acc: 0.9984 - val_loss: 0.3921 - val_acc: 0.9001\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0118 - acc: 0.9990 - val_loss: 0.2839 - val_acc: 0.9249\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 0.0133 - acc: 0.9988 - val_loss: 1.1578 - val_acc: 0.8664\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 70s 350ms/step - loss: 0.0120 - acc: 0.9992 - val_loss: 0.3466 - val_acc: 0.9238\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.4317 - val_acc: 0.9152\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 66s 330ms/step - loss: 0.0115 - acc: 0.9992 - val_loss: 0.3089 - val_acc: 0.9288\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 67s 335ms/step - loss: 0.0118 - acc: 0.9990 - val_loss: 0.2885 - val_acc: 0.9159\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.0128 - acc: 0.9988 - val_loss: 0.2909 - val_acc: 0.9253\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 69s 343ms/step - loss: 0.0131 - acc: 0.9986 - val_loss: 0.5313 - val_acc: 0.9064\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 71s 354ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.4626 - val_acc: 0.9011\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 0.0116 - acc: 0.9991 - val_loss: 0.6740 - val_acc: 0.9120\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0124 - acc: 0.9988 - val_loss: 0.5750 - val_acc: 0.9083\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 67s 336ms/step - loss: 0.0118 - acc: 0.9989 - val_loss: 0.4575 - val_acc: 0.9158\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 69s 345ms/step - loss: 0.0128 - acc: 0.9989 - val_loss: 0.3969 - val_acc: 0.9275\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 69s 343ms/step - loss: 0.0127 - acc: 0.9990 - val_loss: 0.2827 - val_acc: 0.9220\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 66s 332ms/step - loss: 0.0124 - acc: 0.9991 - val_loss: 0.3088 - val_acc: 0.9217\n",
      "Epoch 160/300\n",
      "200/200 [==============================] - 68s 340ms/step - loss: 0.0109 - acc: 0.9994 - val_loss: 0.3291 - val_acc: 0.9232\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 71s 353ms/step - loss: 0.0135 - acc: 0.9986 - val_loss: 0.5695 - val_acc: 0.9107\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 68s 338ms/step - loss: 0.0121 - acc: 0.9987 - val_loss: 0.4121 - val_acc: 0.9066\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 0.0116 - acc: 0.9990 - val_loss: 0.4225 - val_acc: 0.9154\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0121 - acc: 0.9990 - val_loss: 0.3347 - val_acc: 0.9221\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 67s 333ms/step - loss: 0.0118 - acc: 0.9991 - val_loss: 0.3407 - val_acc: 0.9206\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0125 - acc: 0.9987 - val_loss: 0.5058 - val_acc: 0.9205\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0117 - acc: 0.9991 - val_loss: 0.4210 - val_acc: 0.9226\n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.3517 - val_acc: 0.9231\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 64s 322ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.3462 - val_acc: 0.9221\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0117 - acc: 0.9989 - val_loss: 0.3649 - val_acc: 0.9208\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0123 - acc: 0.9989 - val_loss: 0.4044 - val_acc: 0.9145\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0129 - acc: 0.9987 - val_loss: 0.5006 - val_acc: 0.9192\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0126 - acc: 0.9986 - val_loss: 0.4206 - val_acc: 0.9239\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0117 - acc: 0.9992 - val_loss: 0.3602 - val_acc: 0.9188\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3758 - val_acc: 0.9168\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.4285 - val_acc: 0.9071\n",
      "Epoch 177/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0107 - acc: 0.9995 - val_loss: 0.3664 - val_acc: 0.9220\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0125 - acc: 0.9989 - val_loss: 0.4944 - val_acc: 0.9200\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 0.0118 - acc: 0.9988 - val_loss: 0.4015 - val_acc: 0.9179\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0124 - acc: 0.9990 - val_loss: 0.3566 - val_acc: 0.9223\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0115 - acc: 0.9992 - val_loss: 0.3637 - val_acc: 0.9187\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0120 - acc: 0.9991 - val_loss: 0.3892 - val_acc: 0.9103\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0119 - acc: 0.9989 - val_loss: 0.3896 - val_acc: 0.9083\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0124 - acc: 0.9992 - val_loss: 0.3880 - val_acc: 0.9126\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0124 - acc: 0.9989 - val_loss: 0.3512 - val_acc: 0.9248\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0122 - acc: 0.9987 - val_loss: 0.3368 - val_acc: 0.9260\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.0112 - acc: 0.9993 - val_loss: 0.3498 - val_acc: 0.9209\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.3555 - val_acc: 0.9221\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.4282 - val_acc: 0.9134\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0118 - acc: 0.9992 - val_loss: 0.3715 - val_acc: 0.9192\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0120 - acc: 0.9992 - val_loss: 0.3128 - val_acc: 0.9221\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0111 - acc: 0.9993 - val_loss: 0.3743 - val_acc: 0.9141\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0122 - acc: 0.9989 - val_loss: 0.3578 - val_acc: 0.9177\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 0.0130 - acc: 0.9987 - val_loss: 0.3911 - val_acc: 0.9154\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0117 - acc: 0.9992 - val_loss: 0.3740 - val_acc: 0.9217\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 65s 325ms/step - loss: 0.0118 - acc: 0.9992 - val_loss: 0.3026 - val_acc: 0.9249\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0113 - acc: 0.9992 - val_loss: 0.3969 - val_acc: 0.9179\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0124 - acc: 0.9987 - val_loss: 0.4033 - val_acc: 0.9165\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0118 - acc: 0.9991 - val_loss: 0.4121 - val_acc: 0.9090\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.4479 - val_acc: 0.9197\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.3672 - val_acc: 0.9221\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.3508 - val_acc: 0.9183\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0120 - acc: 0.9988 - val_loss: 0.3518 - val_acc: 0.9227\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0117 - acc: 0.9990 - val_loss: 0.4575 - val_acc: 0.9257\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0121 - acc: 0.9992 - val_loss: 0.3640 - val_acc: 0.9259\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 0.0118 - acc: 0.9992 - val_loss: 0.4113 - val_acc: 0.9212\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0117 - acc: 0.9988 - val_loss: 0.3963 - val_acc: 0.9150\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 0.0128 - acc: 0.9988 - val_loss: 0.3499 - val_acc: 0.9245\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0131 - acc: 0.9987 - val_loss: 0.4234 - val_acc: 0.9162\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0113 - acc: 0.9991 - val_loss: 0.4192 - val_acc: 0.9115\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.4468 - val_acc: 0.9177\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.4244 - val_acc: 0.9197\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0116 - acc: 0.9991 - val_loss: 0.3723 - val_acc: 0.9230\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0112 - acc: 0.9994 - val_loss: 0.3610 - val_acc: 0.9195\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.3420 - val_acc: 0.9187\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0120 - acc: 0.9989 - val_loss: 0.3533 - val_acc: 0.9202\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.3511 - val_acc: 0.9208\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3098 - val_acc: 0.9213\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0120 - acc: 0.9993 - val_loss: 0.4125 - val_acc: 0.9108\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0115 - acc: 0.9994 - val_loss: 0.3848 - val_acc: 0.9188\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3326 - val_acc: 0.9251\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0121 - acc: 0.9988 - val_loss: 0.4146 - val_acc: 0.9164\n",
      "Epoch 223/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0111 - acc: 0.9992 - val_loss: 0.4174 - val_acc: 0.9207\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 0.0117 - acc: 0.9993 - val_loss: 0.4480 - val_acc: 0.9122\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0121 - acc: 0.9988 - val_loss: 0.3255 - val_acc: 0.9224\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.3308 - val_acc: 0.9223\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0119 - acc: 0.9989 - val_loss: 0.3798 - val_acc: 0.9189\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0119 - acc: 0.9989 - val_loss: 0.4385 - val_acc: 0.9134\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0120 - acc: 0.9991 - val_loss: 0.3500 - val_acc: 0.9235\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3610 - val_acc: 0.9206\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 0.0117 - acc: 0.9992 - val_loss: 0.3818 - val_acc: 0.9222\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0118 - acc: 0.9990 - val_loss: 0.3603 - val_acc: 0.9242\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 0.0117 - acc: 0.9988 - val_loss: 0.3759 - val_acc: 0.9146\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 69s 343ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.3741 - val_acc: 0.9173\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3865 - val_acc: 0.9107\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 70s 352ms/step - loss: 0.0113 - acc: 0.9995 - val_loss: 0.3303 - val_acc: 0.9202\n",
      "Epoch 237/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0118 - acc: 0.9991 - val_loss: 0.3549 - val_acc: 0.9169\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 0.0115 - acc: 0.9993 - val_loss: 0.3992 - val_acc: 0.9162\n",
      "Epoch 239/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0117 - acc: 0.9990 - val_loss: 0.4196 - val_acc: 0.9081\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 69s 346ms/step - loss: 0.0109 - acc: 0.9993 - val_loss: 0.3320 - val_acc: 0.9192\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0124 - acc: 0.9987 - val_loss: 0.3483 - val_acc: 0.9134\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.0115 - acc: 0.9991 - val_loss: 0.3212 - val_acc: 0.9289\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0127 - acc: 0.9988 - val_loss: 0.3354 - val_acc: 0.9217\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.3519 - val_acc: 0.9235\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0118 - acc: 0.9990 - val_loss: 0.3818 - val_acc: 0.9220\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0121 - acc: 0.9990 - val_loss: 0.3329 - val_acc: 0.9166\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.4268 - val_acc: 0.9218\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.4443 - val_acc: 0.9191\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0117 - acc: 0.9990 - val_loss: 0.4209 - val_acc: 0.9184\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0118 - acc: 0.9993 - val_loss: 0.3314 - val_acc: 0.9240\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.4139 - val_acc: 0.9189\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 63s 316ms/step - loss: 0.0120 - acc: 0.9990 - val_loss: 0.4124 - val_acc: 0.9172\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0113 - acc: 0.9993 - val_loss: 0.4166 - val_acc: 0.9141\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 0.0116 - acc: 0.9990 - val_loss: 0.3878 - val_acc: 0.9191\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0119 - acc: 0.9989 - val_loss: 0.4352 - val_acc: 0.9162\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.3851 - val_acc: 0.9199\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0118 - acc: 0.9991 - val_loss: 0.3274 - val_acc: 0.9203\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 0.0115 - acc: 0.9991 - val_loss: 0.4171 - val_acc: 0.9142\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0113 - acc: 0.9992 - val_loss: 0.3703 - val_acc: 0.9216\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0121 - acc: 0.9987 - val_loss: 0.3642 - val_acc: 0.9178\n",
      "Epoch 261/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.3539 - val_acc: 0.9227\n",
      "Epoch 262/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.3100 - val_acc: 0.9268\n",
      "Epoch 263/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0110 - acc: 0.9992 - val_loss: 0.4273 - val_acc: 0.9200\n",
      "Epoch 264/300\n",
      "200/200 [==============================] - 64s 319ms/step - loss: 0.0114 - acc: 0.9992 - val_loss: 0.3835 - val_acc: 0.9212\n",
      "Epoch 265/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0123 - acc: 0.9984 - val_loss: 0.4177 - val_acc: 0.9161\n",
      "Epoch 266/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0113 - acc: 0.9992 - val_loss: 0.3388 - val_acc: 0.9180\n",
      "Epoch 267/300\n",
      "200/200 [==============================] - 61s 306ms/step - loss: 0.0109 - acc: 0.9992 - val_loss: 0.3229 - val_acc: 0.9199\n",
      "Epoch 268/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0116 - acc: 0.9991 - val_loss: 0.3671 - val_acc: 0.9232\n",
      "Epoch 269/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0108 - acc: 0.9995 - val_loss: 0.4254 - val_acc: 0.9136\n",
      "Epoch 270/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0115 - acc: 0.9988 - val_loss: 0.3548 - val_acc: 0.9245\n",
      "Epoch 271/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.4009 - val_acc: 0.9134\n",
      "Epoch 272/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0116 - acc: 0.9993 - val_loss: 0.3894 - val_acc: 0.9134\n",
      "Epoch 273/300\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 0.0116 - acc: 0.9992 - val_loss: 0.3354 - val_acc: 0.9171\n",
      "Epoch 274/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.3002 - val_acc: 0.9229\n",
      "Epoch 275/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0119 - acc: 0.9990 - val_loss: 0.3264 - val_acc: 0.9215\n",
      "Epoch 276/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0121 - acc: 0.9989 - val_loss: 0.3316 - val_acc: 0.9255\n",
      "Epoch 277/300\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 0.0117 - acc: 0.9989 - val_loss: 0.4152 - val_acc: 0.9169\n",
      "Epoch 278/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0111 - acc: 0.9993 - val_loss: 0.3728 - val_acc: 0.9171\n",
      "Epoch 279/300\n",
      "200/200 [==============================] - 62s 310ms/step - loss: 0.0122 - acc: 0.9988 - val_loss: 0.3857 - val_acc: 0.9185\n",
      "Epoch 280/300\n",
      "200/200 [==============================] - 64s 321ms/step - loss: 0.0111 - acc: 0.9991 - val_loss: 0.3974 - val_acc: 0.9157\n",
      "Epoch 281/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0121 - acc: 0.9990 - val_loss: 0.3954 - val_acc: 0.9223\n",
      "Epoch 282/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0126 - acc: 0.9988 - val_loss: 0.3449 - val_acc: 0.9257\n",
      "Epoch 283/300\n",
      "200/200 [==============================] - 61s 305ms/step - loss: 0.0109 - acc: 0.9994 - val_loss: 0.3459 - val_acc: 0.9233\n",
      "Epoch 284/300\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 0.0118 - acc: 0.9989 - val_loss: 0.4373 - val_acc: 0.9149\n",
      "Epoch 285/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.4120 - val_acc: 0.9201\n",
      "Epoch 286/300\n",
      "200/200 [==============================] - 65s 324ms/step - loss: 0.0116 - acc: 0.9991 - val_loss: 0.4148 - val_acc: 0.9258\n",
      "Epoch 287/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0113 - acc: 0.9991 - val_loss: 0.3959 - val_acc: 0.9246\n",
      "Epoch 288/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0116 - acc: 0.9992 - val_loss: 0.4085 - val_acc: 0.9175\n",
      "Epoch 289/300\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 0.0119 - acc: 0.9988 - val_loss: 0.3665 - val_acc: 0.9195\n",
      "Epoch 290/300\n",
      "200/200 [==============================] - 63s 315ms/step - loss: 0.0124 - acc: 0.9989 - val_loss: 0.3573 - val_acc: 0.9270\n",
      "Epoch 291/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0111 - acc: 0.9991 - val_loss: 0.3461 - val_acc: 0.9240\n",
      "Epoch 292/300\n",
      "200/200 [==============================] - 63s 314ms/step - loss: 0.0118 - acc: 0.9992 - val_loss: 0.4324 - val_acc: 0.9146\n",
      "Epoch 293/300\n",
      "200/200 [==============================] - 61s 307ms/step - loss: 0.0113 - acc: 0.9993 - val_loss: 0.3725 - val_acc: 0.9229\n",
      "Epoch 294/300\n",
      "200/200 [==============================] - 64s 318ms/step - loss: 0.0114 - acc: 0.9991 - val_loss: 0.3914 - val_acc: 0.9225\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0110 - acc: 0.9994 - val_loss: 0.4009 - val_acc: 0.9173\n",
      "Epoch 296/300\n",
      "200/200 [==============================] - 62s 312ms/step - loss: 0.0118 - acc: 0.9990 - val_loss: 0.4172 - val_acc: 0.9155\n",
      "Epoch 297/300\n",
      "200/200 [==============================] - 62s 308ms/step - loss: 0.0114 - acc: 0.9989 - val_loss: 0.3636 - val_acc: 0.9199\n",
      "Epoch 298/300\n",
      "200/200 [==============================] - 63s 313ms/step - loss: 0.0111 - acc: 0.9992 - val_loss: 0.3681 - val_acc: 0.9229\n",
      "Epoch 299/300\n",
      "200/200 [==============================] - 62s 311ms/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.4922 - val_acc: 0.9197\n",
      "Epoch 300/300\n",
      "200/200 [==============================] - 63s 317ms/step - loss: 0.0112 - acc: 0.9991 - val_loss: 0.3136 - val_acc: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# optimizer = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model_top.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience=100), \n",
    "                 ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n",
    "output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n",
    "                                  validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [00:53<00:00,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_score50 = []\n",
    "output_class50 = []\n",
    "answer_class50 = []\n",
    "answer_class50_1 =[]\n",
    "\n",
    "for i in trange(len(test50_generator)):\n",
    "    output50 = model_top.predict_on_batch(test50_generator[i][0])\n",
    "    output_score50.append(output50)\n",
    "    answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "output_score50 = np.concatenate(output_score50)\n",
    "answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "output_class50 = np.argmax(output_score50, axis=1)\n",
    "answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "print(output_class50)\n",
    "print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89     18788\n",
      "           1       0.88      0.90      0.89     18778\n",
      "\n",
      "    accuracy                           0.89     37566\n",
      "   macro avg       0.89      0.89      0.89     37566\n",
      "weighted avg       0.89      0.89      0.89     37566\n",
      "\n",
      "[[16409  2379]\n",
      " [ 1792 16986]]\n",
      "AUROC: 0.952616\n",
      "0.7261521220224199\n",
      "test_acc:  0.8889687483362615\n"
     ]
    }
   ],
   "source": [
    "cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "print(report50)\n",
    "print(cm50)\n",
    "print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "print(thresh50)\n",
    "print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
