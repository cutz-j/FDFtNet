{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, Lambda, Dropout\n",
    "from keras.layers import SeparableConv2D, Add, Convolution2D, concatenate, Layer, ReLU, DepthwiseConv2D, Reshape, Multiply, InputSpec\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "from keras.applications import Xception, ResNet152\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2  # number of classes\n",
    "img_width, img_height = 64, 64  # change based on the shape/structure of your images\n",
    "batch_size = 64  # try 4, 8, 16, 32, 64, 128, 256 dependent on CPU/GPU memory capacity (powers of 2 values).\n",
    "nb_epoch = 300  # number of iteration the algorithm gets trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/mnt/a/fakedata/face2face/train'\n",
    "validation_dir = '/mnt/a/fakedata/face2face/val'\n",
    "test50_dir = '/mnt/a/fakedata/face2face/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Loaded Model from disk\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 32)        1056      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 60, 60, 32)        1056      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 64)        4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        4160      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 32)        4128      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,835,106\n",
      "Trainable params: 4,833,058\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ShallowNet V3\n",
    "img_input = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# block 1\n",
    "x = Conv2D(32, (5, 5), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(img_input)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 2\n",
    "x = Conv2D(64, (3, 3), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(64, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(64, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 3\n",
    "x = Conv2D(128, (3, 3), padding='valid', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Conv2D(32, (1, 1), kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "x = Activation('relu')(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "\n",
    "# block 4\n",
    "x_flatten = Flatten()(x)\n",
    "x_de = Dense(1024, kernel_regularizer=regularizers.l2(0.0001))(x_flatten)\n",
    "x_rl = Activation('relu')(x_de)\n",
    "x_bn = BatchNormalization()(x_rl)\n",
    "\n",
    "x_sig = Dense(2, activation=None)(x_bn)\n",
    "out = Activation('sigmoid')(x_sig)\n",
    "\n",
    "model = Model(img_input, out)\n",
    "print(\"Loaded Model from disk\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback_list = [EarlyStopping(monitor='val_accuracy', patience=10),\n",
    "#                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "# history = model.fit_generator(train_generator,\n",
    "#                             steps_per_epoch=200,\n",
    "#                             epochs=100,\n",
    "#                             validation_data=validation_generator,\n",
    "#                             validation_steps=len(validation_generator),\n",
    "#                             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/www/fake_detection/model/face2face_shallownet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('/home/www/fake_detection/model/face2face_shallownet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.predict_generator(test50_generator, steps=len(test50_generator), verbose=1)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# print(test50_generator.class_indices)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_score50 = []\n",
    "# output_class50 = []\n",
    "# answer_class50 = []\n",
    "# answer_class50_1 =[]\n",
    "\n",
    "# for i in trange(len(test50_generator)):\n",
    "#     output50 = model.predict_on_batch(test50_generator[i][0])\n",
    "#     output_score50.append(output50)\n",
    "#     answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "# output_score50 = np.concatenate(output_score50)\n",
    "# answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "# output_class50 = np.argmax(output_score50, axis=1)\n",
    "# answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "# print(output_class50)\n",
    "# print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "# report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "# recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "# fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "# fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "# eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "# thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "# print(report50)\n",
    "# print(cm50)\n",
    "# print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "# print(thresh50)\n",
    "# print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(img):\n",
    "    \"\"\"\n",
    "    # Function: RandomCrop (ZeroPadded (4, 4)) + random occulusion image\n",
    "    # Arguments:\n",
    "        img: image\n",
    "    # Returns:\n",
    "        img\n",
    "    \"\"\"\n",
    "    img = bgr(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    channels = img.shape[2]\n",
    "    MAX_CUTS = 3 # chance to get more cuts\n",
    "    MAX_LENGTH_MUTIPLIER = 10 # chance to get larger cuts\n",
    "    # 16 for cifar10, 8 for cifar100\n",
    "    \n",
    "    # Zero-padded (4, 4)\n",
    "#     img = np.pad(img, ((4,4),(4,4),(0,0)), mode='constant', constant_values=(0))\n",
    "    \n",
    "#     # random-crop 64x64\n",
    "#     dy, dx = height, width\n",
    "#     x = np.random.randint(0, width - dx + 1)\n",
    "#     y = np.random.randint(0, height - dy + 1)\n",
    "#     img = img[y:(y+dy), x:(x+dx)]\n",
    "    \n",
    "#     mean norm\n",
    "#     mean = img.mean(keepdims=True)\n",
    "#     img -= mean\n",
    "\n",
    "    img *= 1./255\n",
    "    \n",
    "    mask = np.ones((height, width, channels), dtype=np.float32)\n",
    "    nb_cuts = np.random.randint(0, MAX_CUTS + 1)\n",
    "    \n",
    "    # cutout\n",
    "    for i in range(nb_cuts):\n",
    "        y = np.random.randint(height)\n",
    "        x = np.random.randint(width)\n",
    "        length = 4 * np.random.randint(1, MAX_LENGTH_MUTIPLIER+1)\n",
    "        \n",
    "        y1 = np.clip(y-length//2, 0, height)\n",
    "        y2 = np.clip(y+length//2, 0, height)\n",
    "        x1 = np.clip(x-length//2, 0, width)\n",
    "        x2 = np.clip(x+length//2, 0, width)\n",
    "        \n",
    "        mask[y1:y2, x1:x2, :] = 0.\n",
    "    \n",
    "    img = img * mask\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU6(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"ReLU6\")\n",
    "        self.relu6 = ReLU(max_value=6, name=\"ReLU6\")\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input)\n",
    "\n",
    "\n",
    "class HardSigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu6 = ReLU6()\n",
    "\n",
    "    def call(self, input):\n",
    "        return self.relu6(input + 3.0) / 6.0\n",
    "\n",
    "\n",
    "class HardSwish(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hard_sigmoid = HardSigmoid()\n",
    "\n",
    "    def call(self, input):\n",
    "        return input * self.hard_sigmoid(input)\n",
    "    \n",
    "class Attention(Layer):\n",
    "    def __init__(self, ch, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.channels = ch\n",
    "        self.filters_f_g = self.channels // 8\n",
    "        self.filters_h = self.channels\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n",
    "        print(kernel_shape_f_g)\n",
    "        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n",
    "\n",
    "        # Create a trainable weight variable for this layer:\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n",
    "        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_f')\n",
    "        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_g')\n",
    "        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='kernel_h')\n",
    "        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_F')\n",
    "        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_g')\n",
    "        self.bias_h = self.add_weight(shape=(self.filters_h,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_h')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=4,\n",
    "                                    axes={3: input_shape[-1]})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        def hw_flatten(x):\n",
    "            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n",
    "\n",
    "        f = K.conv2d(x,\n",
    "                     kernel=self.kernel_f,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        f = K.bias_add(f, self.bias_f)\n",
    "        g = K.conv2d(x,\n",
    "                     kernel=self.kernel_g,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n",
    "        g = K.bias_add(g, self.bias_g)\n",
    "        h = K.conv2d(x,\n",
    "                     kernel=self.kernel_h,\n",
    "                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n",
    "        h = K.bias_add(h, self.bias_h)\n",
    "\n",
    "        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n",
    "\n",
    "        beta = K.softmax(s, axis=-1)  # attention map\n",
    "\n",
    "        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n",
    "\n",
    "        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n",
    "        x = self.gamma * o + x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 18000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "ft_dir = '/mnt/a/fakedata/face2face/finetune'\n",
    "train_gen_aug = ImageDataGenerator(shear_range=0, \n",
    "                               zoom_range=0.2,\n",
    "                               rotation_range=0.2,\n",
    "                               width_shift_range=2, \n",
    "                               height_shift_range=2,\n",
    "                               horizontal_flip=True,\n",
    "                               zca_whitening=False,\n",
    "                               fill_mode='nearest',\n",
    "                               preprocessing_function=cutout)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=bgr)\n",
    "\n",
    "ft_gen = train_gen_aug.flow_from_directory(ft_dir,\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=(img_height, img_width),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        class_mode='categorical')\n",
    "\n",
    "test50_generator = test_datagen.flow_from_directory(test50_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/www/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "(1, 1, 32, 4)\n",
      "(1, 1, 64, 8)\n",
      "(1, 1, 128, 16)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 12, 12, 32)   109344      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 12, 12, 1024) 32768       model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 12, 12, 1024) 4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_1 (HardSwish)        (None, 12, 12, 1024) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv2d_1 (DepthwiseCo (None, 6, 6, 1024)   9216        hard_swish_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 32, 32, 32)   123         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 6, 6, 1024)   4096        depthwise_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 32)   128         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1024)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 32, 32, 32)   1321        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 1, 1, 256)    262144      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 16, 16, 64)   2336        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 1, 1, 256)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 1, 1, 1024)   262144      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "hard_sigmoid_2 (HardSigmoid)    (None, 1, 1, 1024)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 16, 16, 64)   5201        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 12, 12, 1024) 0           hard_swish_1[0][0]               \n",
      "                                                                 hard_sigmoid_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 8, 8, 128)    8768        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_2 (HardSwish)        (None, 12, 12, 1024) 0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 12, 12, 256)  262144      hard_swish_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 12, 12, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 8, 8, 128)    20641       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 12, 12, 576)  147456      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 576)    73728       attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 12, 12, 576)  2304        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 576)    2304        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hard_swish_9 (HardSwish)        (None, 12, 12, 576)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 576)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 576)          0           hard_swish_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 576)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 576)          0           global_average_pooling2d_5[0][0] \n",
      "                                                                 global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1154        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 2)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,213,208\n",
      "Trainable params: 1,205,848\n",
      "Non-trainable params: 7,360\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model('/home/www/fake_detection/model/face2face_shallownet.h5')\n",
    "for i in range(6):\n",
    "    model_ft.layers.pop()\n",
    "im_in = Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "base_model = Model(img_input, x)\n",
    "base_model.set_weights(model_ft.get_weights())\n",
    "# for i in range(len(base_model.layers) - 0):\n",
    "#     base_model.layers[i].trainable = False\n",
    "    \n",
    "x1 = base_model(im_in) # (12, 12, 32)\n",
    "########### Mobilenet block bneck 3x3 (32 --> 128) #################\n",
    "expand1 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(x1)\n",
    "expand1 = BatchNormalization()(expand1)\n",
    "expand1 = HardSwish()(expand1)\n",
    "dw1 = DepthwiseConv2D(kernel_size=(3,3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand1)\n",
    "dw1 = BatchNormalization()(dw1)\n",
    "se_gap1 = GlobalAveragePooling2D()(dw1)\n",
    "se_gap1 = Reshape([1, 1, -1])(se_gap1)\n",
    "se1 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap1)\n",
    "se1 = Activation('relu')(se1)\n",
    "se1 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se1)\n",
    "se1 = HardSigmoid()(se1)\n",
    "se1 = Multiply()([expand1, se1])\n",
    "project1 = HardSwish()(se1)\n",
    "project1 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "project1 = BatchNormalization()(project1)\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand2 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "expand2 = BatchNormalization()(expand2)\n",
    "expand2 = HardSwish()(expand2)\n",
    "dw2 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand2)\n",
    "dw2 = BatchNormalization()(dw2)\n",
    "se_gap2 = GlobalAveragePooling2D()(dw2)\n",
    "se_gap2 = Reshape([1, 1, -1])(se_gap2)\n",
    "se2 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap2)\n",
    "se2 = Activation('relu')(se2)\n",
    "se2 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se2)\n",
    "se2 = HardSigmoid()(se2)\n",
    "se2 = Multiply()([expand2, se2])\n",
    "project2 = HardSwish()(se2)\n",
    "project2 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "project2 = BatchNormalization()(project2)\n",
    "project2 = Add()([project1, project2])\n",
    "\n",
    "########### Mobilenet block bneck 5x5 (128 --> 128) #################\n",
    "expand3 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project2)\n",
    "expand3 = BatchNormalization()(expand3)\n",
    "expand3 = HardSwish()(expand3)\n",
    "dw3 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand3)\n",
    "dw3 = BatchNormalization()(dw3)\n",
    "se_gap3 = GlobalAveragePooling2D()(dw3)\n",
    "se_gap3 = Reshape([1, 1, -1])(se_gap3)\n",
    "se3 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap3)\n",
    "se3 = Activation('relu')(se3)\n",
    "se3 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se3)\n",
    "se3 = HardSigmoid()(se3)\n",
    "se3 = Multiply()([expand3, se3])\n",
    "project3 = HardSwish()(se3)\n",
    "project3 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "project3 = BatchNormalization()(project3)\n",
    "project3 = Add()([project2, project3])\n",
    "\n",
    "\n",
    "expand4 = Conv2D(1024, kernel_size=1, strides=1, kernel_regularizer=l2(1e-5), use_bias=False)(project3)\n",
    "expand4 = BatchNormalization()(expand4)\n",
    "expand4 = HardSwish()(expand4)\n",
    "dw4 = DepthwiseConv2D(kernel_size=(5,5), strides=(1,1), padding='same', depthwise_regularizer=l2(1e-5), use_bias=False)(expand4)\n",
    "dw4 = BatchNormalization()(dw4)\n",
    "se_gap4 = GlobalAveragePooling2D()(dw4)\n",
    "se_gap4 = Reshape([1, 1, -1])(se_gap4)\n",
    "se4 = Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se_gap4)\n",
    "se4 = Activation('relu')(se4)\n",
    "se4 = Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(se4)\n",
    "se4 = HardSigmoid()(se4)\n",
    "se4 = Multiply()([expand4, se4])\n",
    "project4 = HardSwish()(se4)\n",
    "project4 = Conv2D(256, kernel_size=(1, 1), padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project4)\n",
    "project4 = BatchNormalization()(project4)\n",
    "project4 = Add()([project3, project4])\n",
    "\n",
    "\n",
    "########## Classification ##########\n",
    "x2 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(project1)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = HardSwish()(x2)\n",
    "x2 = GlobalAveragePooling2D()(x2)\n",
    "\n",
    "\n",
    "######### Image Attention Model #########\n",
    "### Block 1 ###\n",
    "x3 = SeparableConv2D(32, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(im_in)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "x3 = Attention(32)(x3)\n",
    "\n",
    "### Block 2 ###\n",
    "x4 = SeparableConv2D(64, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x3)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Activation('relu')(x4)\n",
    "x4 = Attention(64)(x4)\n",
    "\n",
    "### Block 3 ###\n",
    "x5 = SeparableConv2D(128, kernel_size=(3, 3), strides=(2,2), padding='same', depthwise_regularizer=l2(1e-5), pointwise_regularizer=l2(1e-5), use_bias=False)(x4)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Activation('relu')(x5)\n",
    "x5 = Attention(128)(x5)\n",
    "\n",
    "### final stage ###\n",
    "x6 = Conv2D(576, kernel_size=1, strides=1, padding='valid', kernel_regularizer=l2(1e-5), use_bias=False)(x5)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Activation('relu')(x6)\n",
    "x6 = GlobalAveragePooling2D()(x6)\n",
    "# x6 = Reshape([1, 1, -1])(x6)\n",
    "\n",
    "######## final addition #########\n",
    "\n",
    "x2 = Add()([x2, x6])\n",
    "x2 = Dense(2)(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_top = Model(inputs=im_in, outputs=x2)\n",
    "model_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "200/200 [==============================] - 92s 459ms/step - loss: 0.7386 - acc: 0.5049 - val_loss: 0.4395 - val_acc: 0.5000\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 0.7150 - acc: 0.5294 - val_loss: 0.3589 - val_acc: 0.5000\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 0.7091 - acc: 0.5408 - val_loss: 0.6185 - val_acc: 0.5516\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 89s 443ms/step - loss: 0.7121 - acc: 0.5382 - val_loss: 0.3039 - val_acc: 0.5000\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 87s 435ms/step - loss: 0.7038 - acc: 0.5475 - val_loss: 0.3980 - val_acc: 0.5024\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 86s 430ms/step - loss: 0.6977 - acc: 0.5571 - val_loss: 0.8288 - val_acc: 0.5562\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.6978 - acc: 0.5565 - val_loss: 1.5042 - val_acc: 0.5021\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.6954 - acc: 0.5592 - val_loss: 0.4932 - val_acc: 0.5469\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 88s 442ms/step - loss: 0.6909 - acc: 0.5693 - val_loss: 1.3447 - val_acc: 0.5167\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 0.6906 - acc: 0.5636 - val_loss: 0.7127 - val_acc: 0.5583\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.6886 - acc: 0.5693 - val_loss: 0.9409 - val_acc: 0.5346\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 0.6860 - acc: 0.5727 - val_loss: 0.7603 - val_acc: 0.5925\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 88s 439ms/step - loss: 0.6806 - acc: 0.5829 - val_loss: 0.1249 - val_acc: 0.5001\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 95s 476ms/step - loss: 0.6856 - acc: 0.5723 - val_loss: 1.2924 - val_acc: 0.5175\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 95s 474ms/step - loss: 0.6781 - acc: 0.5823 - val_loss: 0.8002 - val_acc: 0.5989\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 98s 491ms/step - loss: 0.6768 - acc: 0.5834 - val_loss: 0.7703 - val_acc: 0.5989\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 102s 510ms/step - loss: 0.6754 - acc: 0.5916 - val_loss: 0.4135 - val_acc: 0.5591\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 126s 628ms/step - loss: 0.6711 - acc: 0.5920 - val_loss: 0.7419 - val_acc: 0.6202\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 147s 736ms/step - loss: 0.6685 - acc: 0.5911 - val_loss: 0.6748 - val_acc: 0.6327\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 152s 761ms/step - loss: 0.6639 - acc: 0.5977 - val_loss: 0.3122 - val_acc: 0.5811\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 153s 767ms/step - loss: 0.6618 - acc: 0.6048 - val_loss: 0.1888 - val_acc: 0.5100\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 149s 744ms/step - loss: 0.6616 - acc: 0.5982 - val_loss: 0.3784 - val_acc: 0.5702\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 148s 738ms/step - loss: 0.6581 - acc: 0.6028 - val_loss: 0.6896 - val_acc: 0.6367\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 149s 744ms/step - loss: 0.6537 - acc: 0.6125 - val_loss: 0.8148 - val_acc: 0.6249\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 149s 745ms/step - loss: 0.6487 - acc: 0.6139 - val_loss: 0.9553 - val_acc: 0.5978\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 153s 765ms/step - loss: 0.6507 - acc: 0.6098 - val_loss: 18.6180 - val_acc: 0.5000\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 145s 723ms/step - loss: 0.6437 - acc: 0.6165 - val_loss: 0.3253 - val_acc: 0.5639\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 148s 740ms/step - loss: 0.6413 - acc: 0.6228 - val_loss: 0.2574 - val_acc: 0.5506\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 152s 762ms/step - loss: 0.6423 - acc: 0.6192 - val_loss: 1.2277 - val_acc: 0.5902\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 149s 745ms/step - loss: 0.6328 - acc: 0.6268 - val_loss: 0.3235 - val_acc: 0.5680\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 145s 723ms/step - loss: 0.6291 - acc: 0.6327 - val_loss: 1.8724 - val_acc: 0.5574\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 148s 742ms/step - loss: 0.6336 - acc: 0.6296 - val_loss: 0.4907 - val_acc: 0.6541\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 146s 732ms/step - loss: 0.6302 - acc: 0.6307 - val_loss: 0.8206 - val_acc: 0.6307\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 149s 745ms/step - loss: 0.6317 - acc: 0.6361 - val_loss: 5.5693 - val_acc: 0.5018\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.6262 - acc: 0.6387 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 149s 746ms/step - loss: 0.6198 - acc: 0.6471 - val_loss: 1.5366 - val_acc: 0.5635\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 150s 752ms/step - loss: 0.6223 - acc: 0.6407 - val_loss: 0.6262 - val_acc: 0.6788\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 150s 750ms/step - loss: 0.6183 - acc: 0.6435 - val_loss: 0.9813 - val_acc: 0.6382\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 146s 730ms/step - loss: 0.6133 - acc: 0.6546 - val_loss: 0.9586 - val_acc: 0.6501\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.6171 - acc: 0.6446 - val_loss: 0.6589 - val_acc: 0.6860\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 150s 748ms/step - loss: 0.6135 - acc: 0.6478 - val_loss: 0.6583 - val_acc: 0.6952\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 152s 759ms/step - loss: 0.6089 - acc: 0.6509 - val_loss: 0.2146 - val_acc: 0.5623\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 145s 725ms/step - loss: 0.6090 - acc: 0.6519 - val_loss: 0.8243 - val_acc: 0.6540\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 148s 741ms/step - loss: 0.6023 - acc: 0.6546 - val_loss: 0.2766 - val_acc: 0.6001\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 148s 742ms/step - loss: 0.5996 - acc: 0.6638 - val_loss: 1.4285 - val_acc: 0.5814\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 150s 749ms/step - loss: 0.6026 - acc: 0.6574 - val_loss: 0.0065 - val_acc: 0.5000\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 149s 745ms/step - loss: 0.6081 - acc: 0.6492 - val_loss: 0.7997 - val_acc: 0.6914\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 150s 750ms/step - loss: 0.5984 - acc: 0.6684 - val_loss: 0.7454 - val_acc: 0.6859\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 148s 740ms/step - loss: 0.5982 - acc: 0.6646 - val_loss: 1.3396 - val_acc: 0.6043\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 145s 727ms/step - loss: 0.5969 - acc: 0.6653 - val_loss: 0.2501 - val_acc: 0.5936\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.5927 - acc: 0.6666 - val_loss: 0.0225 - val_acc: 0.5011\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 146s 728ms/step - loss: 0.5933 - acc: 0.6642 - val_loss: 0.6273 - val_acc: 0.6859\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 147s 733ms/step - loss: 0.5868 - acc: 0.6730 - val_loss: 1.4664 - val_acc: 0.6335\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 149s 746ms/step - loss: 0.5870 - acc: 0.6696 - val_loss: 0.2446 - val_acc: 0.6113\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.5890 - acc: 0.6696 - val_loss: 1.6820 - val_acc: 0.5803\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 147s 734ms/step - loss: 0.5904 - acc: 0.6708 - val_loss: 0.1017 - val_acc: 0.5108\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.5866 - acc: 0.6738 - val_loss: 0.9670 - val_acc: 0.6795\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 147s 733ms/step - loss: 0.5839 - acc: 0.6733 - val_loss: 0.6122 - val_acc: 0.7070\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 148s 739ms/step - loss: 0.5838 - acc: 0.6714 - val_loss: 0.8464 - val_acc: 0.7069\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 150s 752ms/step - loss: 0.5821 - acc: 0.6777 - val_loss: 1.8103 - val_acc: 0.5920\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 144s 718ms/step - loss: 0.5801 - acc: 0.6787 - val_loss: 0.2296 - val_acc: 0.5990\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 150s 751ms/step - loss: 0.5828 - acc: 0.6759 - val_loss: 1.7385 - val_acc: 0.6127\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 148s 741ms/step - loss: 0.5753 - acc: 0.6814 - val_loss: 1.4796 - val_acc: 0.6233\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 148s 742ms/step - loss: 0.5688 - acc: 0.6896 - val_loss: 0.0176 - val_acc: 0.5085\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 149s 747ms/step - loss: 0.5726 - acc: 0.6844 - val_loss: 1.0235 - val_acc: 0.6747\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 148s 741ms/step - loss: 0.5673 - acc: 0.6901 - val_loss: 0.5884 - val_acc: 0.7117\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 149s 744ms/step - loss: 0.5712 - acc: 0.6877 - val_loss: 10.7727 - val_acc: 0.5002\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 149s 746ms/step - loss: 0.5725 - acc: 0.6868 - val_loss: 1.6749 - val_acc: 0.6039\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 147s 735ms/step - loss: 0.5671 - acc: 0.6897 - val_loss: 0.3452 - val_acc: 0.6942\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 134s 668ms/step - loss: 0.5640 - acc: 0.6941 - val_loss: 4.2207 - val_acc: 0.5445\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 0.5669 - acc: 0.6896 - val_loss: 1.0394 - val_acc: 0.6808\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 88s 442ms/step - loss: 0.5618 - acc: 0.6954 - val_loss: 0.4520 - val_acc: 0.7107\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 87s 435ms/step - loss: 0.5608 - acc: 0.6989 - val_loss: 0.1478 - val_acc: 0.5943\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 0.5538 - acc: 0.6996 - val_loss: 1.4144 - val_acc: 0.6320\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 0.5619 - acc: 0.6948 - val_loss: 0.6331 - val_acc: 0.7287\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 89s 443ms/step - loss: 0.5517 - acc: 0.6992 - val_loss: 0.6324 - val_acc: 0.7121\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 89s 443ms/step - loss: 0.5566 - acc: 0.6979 - val_loss: 0.2837 - val_acc: 0.7053\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.5539 - acc: 0.7018 - val_loss: 2.8546 - val_acc: 0.5726\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 88s 438ms/step - loss: 0.5508 - acc: 0.7050 - val_loss: 1.8426 - val_acc: 0.6022\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 89s 447ms/step - loss: 0.5527 - acc: 0.7044 - val_loss: 1.1441 - val_acc: 0.6662\n",
      "Epoch 81/300\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 0.5472 - acc: 0.7055 - val_loss: 0.5891 - val_acc: 0.7308\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 0.5475 - acc: 0.7112 - val_loss: 0.0694 - val_acc: 0.5497\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.5474 - acc: 0.7082 - val_loss: 0.0824 - val_acc: 0.5663\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.5389 - acc: 0.7110 - val_loss: 0.4550 - val_acc: 0.7333\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.5439 - acc: 0.7148 - val_loss: 0.1174 - val_acc: 0.5495\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.5461 - acc: 0.7072 - val_loss: 2.9822 - val_acc: 0.5667\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 84s 418ms/step - loss: 0.5331 - acc: 0.7218 - val_loss: 1.6750 - val_acc: 0.6045\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.5433 - acc: 0.7103 - val_loss: 0.1212 - val_acc: 0.5481\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.5387 - acc: 0.7155 - val_loss: 1.7320 - val_acc: 0.6283\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 102s 512ms/step - loss: 0.5415 - acc: 0.7157 - val_loss: 0.4891 - val_acc: 0.7419\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.5347 - acc: 0.7190 - val_loss: 0.7117 - val_acc: 0.7194\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 136s 680ms/step - loss: 0.5330 - acc: 0.7212 - val_loss: 0.3979 - val_acc: 0.7440\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 137s 683ms/step - loss: 0.5324 - acc: 0.7215 - val_loss: 0.9042 - val_acc: 0.6857\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 141s 707ms/step - loss: 0.5282 - acc: 0.7233 - val_loss: 1.6927 - val_acc: 0.6032\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 137s 683ms/step - loss: 0.5322 - acc: 0.7180 - val_loss: 0.3483 - val_acc: 0.7492\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 139s 695ms/step - loss: 0.5223 - acc: 0.7273 - val_loss: 0.4627 - val_acc: 0.7503\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 0.5264 - acc: 0.7272 - val_loss: 0.0714 - val_acc: 0.6133\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 140s 700ms/step - loss: 0.5178 - acc: 0.7330 - val_loss: 0.8705 - val_acc: 0.7328\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 137s 686ms/step - loss: 0.5158 - acc: 0.7338 - val_loss: 1.3290 - val_acc: 0.6457\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.5190 - acc: 0.7290 - val_loss: 0.0272 - val_acc: 0.5364\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 141s 703ms/step - loss: 0.5158 - acc: 0.7347 - val_loss: 0.5550 - val_acc: 0.7542\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 135s 674ms/step - loss: 0.5171 - acc: 0.7372 - val_loss: 5.2804 - val_acc: 0.5351\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.5126 - acc: 0.7383 - val_loss: 0.6511 - val_acc: 0.7393\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 137s 684ms/step - loss: 0.5032 - acc: 0.7439 - val_loss: 0.1632 - val_acc: 0.6787\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 0.5015 - acc: 0.7426 - val_loss: 0.0107 - val_acc: 0.5032\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 136s 681ms/step - loss: 0.5019 - acc: 0.7446 - val_loss: 0.9088 - val_acc: 0.7203\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 136s 681ms/step - loss: 0.4879 - acc: 0.7534 - val_loss: 0.1670 - val_acc: 0.6953\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 136s 678ms/step - loss: 0.4897 - acc: 0.7546 - val_loss: 0.0178 - val_acc: 0.5373\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.4997 - acc: 0.7534 - val_loss: 0.1685 - val_acc: 0.6056\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 144s 719ms/step - loss: 0.4966 - acc: 0.7571 - val_loss: 0.0912 - val_acc: 0.5206\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 136s 680ms/step - loss: 0.4821 - acc: 0.7575 - val_loss: 1.4266 - val_acc: 0.6693\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 128s 642ms/step - loss: 0.4816 - acc: 0.7634 - val_loss: 0.1320 - val_acc: 0.6457\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 138s 688ms/step - loss: 0.4715 - acc: 0.7668 - val_loss: 0.2646 - val_acc: 0.7674\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 136s 680ms/step - loss: 0.4731 - acc: 0.7681 - val_loss: 0.0156 - val_acc: 0.5334\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 135s 675ms/step - loss: 0.4788 - acc: 0.7616 - val_loss: 1.4062 - val_acc: 0.6846\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 135s 674ms/step - loss: 0.4799 - acc: 0.7606 - val_loss: 0.6180 - val_acc: 0.7593\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 133s 666ms/step - loss: 0.4624 - acc: 0.7740 - val_loss: 0.3260 - val_acc: 0.7848\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.4790 - acc: 0.7596 - val_loss: 0.1102 - val_acc: 0.5496\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 134s 668ms/step - loss: 0.4616 - acc: 0.7782 - val_loss: 0.5523 - val_acc: 0.7888\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 137s 683ms/step - loss: 0.4696 - acc: 0.7668 - val_loss: 0.1423 - val_acc: 0.7581\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 114s 568ms/step - loss: 0.4588 - acc: 0.7798 - val_loss: 0.1249 - val_acc: 0.6043\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 141s 706ms/step - loss: 0.4585 - acc: 0.7728 - val_loss: 0.0103 - val_acc: 0.5000\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 141s 706ms/step - loss: 0.4572 - acc: 0.7786 - val_loss: 0.0967 - val_acc: 0.5835\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 143s 717ms/step - loss: 0.4518 - acc: 0.7777 - val_loss: 0.0120 - val_acc: 0.5266\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 143s 715ms/step - loss: 0.4520 - acc: 0.7816 - val_loss: 0.0111 - val_acc: 0.5231\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 127s 635ms/step - loss: 0.4556 - acc: 0.7744 - val_loss: 4.1899 - val_acc: 0.5890\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 137s 687ms/step - loss: 0.4535 - acc: 0.7820 - val_loss: 3.2587 - val_acc: 0.6070\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 134s 671ms/step - loss: 0.4494 - acc: 0.7864 - val_loss: 3.5512 - val_acc: 0.5821\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 143s 713ms/step - loss: 0.4456 - acc: 0.7856 - val_loss: 3.2027 - val_acc: 0.5942\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 140s 700ms/step - loss: 0.4500 - acc: 0.7814 - val_loss: 2.3557 - val_acc: 0.6302\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 138s 692ms/step - loss: 0.4539 - acc: 0.7838 - val_loss: 3.1332 - val_acc: 0.5919\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 139s 696ms/step - loss: 0.4463 - acc: 0.7833 - val_loss: 0.7588 - val_acc: 0.7790\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.4387 - acc: 0.7872 - val_loss: 1.2679 - val_acc: 0.6996\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.4476 - acc: 0.7828 - val_loss: 2.7506 - val_acc: 0.5961\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.4453 - acc: 0.7818 - val_loss: 0.0229 - val_acc: 0.5954\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 0.4440 - acc: 0.7888 - val_loss: 2.7227 - val_acc: 0.6258\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 141s 706ms/step - loss: 0.4341 - acc: 0.7927 - val_loss: 3.3353 - val_acc: 0.6011\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 142s 711ms/step - loss: 0.4374 - acc: 0.7900 - val_loss: 0.0711 - val_acc: 0.7134\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4332 - acc: 0.7938 - val_loss: 0.3778 - val_acc: 0.6430\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 0.4346 - acc: 0.7901 - val_loss: 1.4499 - val_acc: 0.6864\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 140s 701ms/step - loss: 0.4234 - acc: 0.8036 - val_loss: 2.5897 - val_acc: 0.6232\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 143s 717ms/step - loss: 0.4352 - acc: 0.7928 - val_loss: 2.0393 - val_acc: 0.6736\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 141s 705ms/step - loss: 0.4293 - acc: 0.7937 - val_loss: 0.6538 - val_acc: 0.8002\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.4340 - acc: 0.7925 - val_loss: 2.2661 - val_acc: 0.6577\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4401 - acc: 0.7892 - val_loss: 1.0622 - val_acc: 0.7802\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 140s 699ms/step - loss: 0.4325 - acc: 0.7952 - val_loss: 0.0111 - val_acc: 0.5001\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.4182 - acc: 0.8029 - val_loss: 0.0157 - val_acc: 0.5771\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4050 - acc: 0.8099 - val_loss: 0.0918 - val_acc: 0.7628\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 135s 676ms/step - loss: 0.4115 - acc: 0.8071 - val_loss: 0.0312 - val_acc: 0.6338\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 141s 706ms/step - loss: 0.4100 - acc: 0.8132 - val_loss: 4.4172 - val_acc: 0.5748\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 141s 704ms/step - loss: 0.4145 - acc: 0.8028 - val_loss: 1.6634 - val_acc: 0.6807\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 141s 703ms/step - loss: 0.4058 - acc: 0.8079 - val_loss: 0.0813 - val_acc: 0.6799\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 138s 691ms/step - loss: 0.4078 - acc: 0.8101 - val_loss: 0.1206 - val_acc: 0.7812\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 139s 693ms/step - loss: 0.3986 - acc: 0.8113 - val_loss: 0.9270 - val_acc: 0.7741\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 136s 682ms/step - loss: 0.3986 - acc: 0.8153 - val_loss: 2.9436 - val_acc: 0.6018\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 139s 697ms/step - loss: 0.4049 - acc: 0.8135 - val_loss: 0.8040 - val_acc: 0.7678\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 138s 690ms/step - loss: 0.4009 - acc: 0.8197 - val_loss: 0.0110 - val_acc: 0.5026\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 137s 685ms/step - loss: 0.3931 - acc: 0.8158 - val_loss: 2.7404 - val_acc: 0.6112\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 111s 554ms/step - loss: 0.3993 - acc: 0.8161 - val_loss: 0.9558 - val_acc: 0.7596\n",
      "Epoch 160/300\n",
      "200/200 [==============================] - 108s 542ms/step - loss: 0.3975 - acc: 0.8147 - val_loss: 3.8444 - val_acc: 0.6159\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 105s 526ms/step - loss: 0.4062 - acc: 0.8104 - val_loss: 0.0113 - val_acc: 0.5103\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 97s 484ms/step - loss: 0.3933 - acc: 0.8198 - val_loss: 1.8664 - val_acc: 0.6738\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 98s 488ms/step - loss: 0.3922 - acc: 0.8197 - val_loss: 1.7827 - val_acc: 0.6989\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 96s 482ms/step - loss: 0.3960 - acc: 0.8159 - val_loss: 0.5060 - val_acc: 0.8234\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3854 - acc: 0.8211 - val_loss: 1.6818 - val_acc: 0.6675\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 95s 474ms/step - loss: 0.3891 - acc: 0.8238 - val_loss: 0.0372 - val_acc: 0.6549\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 97s 487ms/step - loss: 0.3959 - acc: 0.8179 - val_loss: 2.1133 - val_acc: 0.6751\n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 97s 487ms/step - loss: 0.3929 - acc: 0.8175 - val_loss: 0.1724 - val_acc: 0.8192\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 97s 486ms/step - loss: 0.3888 - acc: 0.8222 - val_loss: 0.0115 - val_acc: 0.5369\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3844 - acc: 0.8243 - val_loss: 0.4303 - val_acc: 0.8301\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3843 - acc: 0.8246 - val_loss: 0.1473 - val_acc: 0.8008\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 96s 479ms/step - loss: 0.3889 - acc: 0.8212 - val_loss: 0.0434 - val_acc: 0.7741\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3932 - acc: 0.8160 - val_loss: 2.6461 - val_acc: 0.6400\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 95s 477ms/step - loss: 0.3902 - acc: 0.8228 - val_loss: 1.0881 - val_acc: 0.7640\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 97s 485ms/step - loss: 0.3874 - acc: 0.8194 - val_loss: 0.0274 - val_acc: 0.7598\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 99s 494ms/step - loss: 0.3748 - acc: 0.8268 - val_loss: 1.1573 - val_acc: 0.7451\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 96s 482ms/step - loss: 0.3671 - acc: 0.8311 - val_loss: 0.3537 - val_acc: 0.8324\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 97s 483ms/step - loss: 0.3734 - acc: 0.8281 - val_loss: 1.6277 - val_acc: 0.7158\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3727 - acc: 0.8344 - val_loss: 1.0936 - val_acc: 0.7335\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 96s 481ms/step - loss: 0.3661 - acc: 0.8322 - val_loss: 1.7892 - val_acc: 0.6878\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 94s 471ms/step - loss: 0.3703 - acc: 0.8350 - val_loss: 5.0567 - val_acc: 0.5616\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 96s 482ms/step - loss: 0.3646 - acc: 0.8331 - val_loss: 0.0362 - val_acc: 0.7169\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 98s 488ms/step - loss: 0.3716 - acc: 0.8326 - val_loss: 0.0811 - val_acc: 0.8056\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 96s 480ms/step - loss: 0.3622 - acc: 0.8393 - val_loss: 0.1815 - val_acc: 0.8434\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 95s 477ms/step - loss: 0.3646 - acc: 0.8342 - val_loss: 0.2297 - val_acc: 0.8393\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 98s 488ms/step - loss: 0.3713 - acc: 0.8338 - val_loss: 0.7755 - val_acc: 0.7968\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 94s 468ms/step - loss: 0.3601 - acc: 0.8386 - val_loss: 0.3251 - val_acc: 0.8344\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 0.3631 - acc: 0.8346 - val_loss: 2.8117 - val_acc: 0.6441\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 88s 441ms/step - loss: 0.3528 - acc: 0.8398 - val_loss: 0.2948 - val_acc: 0.8131\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 89s 444ms/step - loss: 0.3574 - acc: 0.8421 - val_loss: 2.2297 - val_acc: 0.6546\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3649 - acc: 0.8385 - val_loss: 0.2458 - val_acc: 0.8423\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 88s 440ms/step - loss: 0.3639 - acc: 0.8342 - val_loss: 0.0161 - val_acc: 0.6706\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 0.3522 - acc: 0.8403 - val_loss: 1.2143 - val_acc: 0.7506\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 0.3610 - acc: 0.8366 - val_loss: 0.8583 - val_acc: 0.7933\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.3531 - acc: 0.8380 - val_loss: 1.5860 - val_acc: 0.7008\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 87s 435ms/step - loss: 0.3547 - acc: 0.8376 - val_loss: 0.1913 - val_acc: 0.8472\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3547 - acc: 0.8366 - val_loss: 0.2998 - val_acc: 0.8407\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 0.3554 - acc: 0.8415 - val_loss: 0.1174 - val_acc: 0.8402\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3522 - acc: 0.8416 - val_loss: 4.4398 - val_acc: 0.5741\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3528 - acc: 0.8388 - val_loss: 1.6617 - val_acc: 0.6878\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 0.3495 - acc: 0.8443 - val_loss: 0.6039 - val_acc: 0.8056\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3498 - acc: 0.8404 - val_loss: 0.1061 - val_acc: 0.8417\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3488 - acc: 0.8411 - val_loss: 0.1072 - val_acc: 0.7817\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 86s 431ms/step - loss: 0.3523 - acc: 0.8423 - val_loss: 1.8151 - val_acc: 0.6812\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.3587 - acc: 0.8412 - val_loss: 1.1788 - val_acc: 0.7689\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3570 - acc: 0.8393 - val_loss: 0.8267 - val_acc: 0.7749\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3442 - acc: 0.8453 - val_loss: 0.0255 - val_acc: 0.7644\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.3421 - acc: 0.8488 - val_loss: 0.2823 - val_acc: 0.8524\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 0.3389 - acc: 0.8445 - val_loss: 0.3575 - val_acc: 0.8392\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3414 - acc: 0.8485 - val_loss: 0.5782 - val_acc: 0.8017\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3426 - acc: 0.8490 - val_loss: 0.3376 - val_acc: 0.8342\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 86s 430ms/step - loss: 0.3344 - acc: 0.8479 - val_loss: 1.2784 - val_acc: 0.7631\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3351 - acc: 0.8525 - val_loss: 0.4665 - val_acc: 0.8261\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.3378 - acc: 0.8485 - val_loss: 0.0615 - val_acc: 0.8394\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3309 - acc: 0.8519 - val_loss: 2.2144 - val_acc: 0.6700\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 0.3320 - acc: 0.8521 - val_loss: 1.0760 - val_acc: 0.7306\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 83s 415ms/step - loss: 0.3435 - acc: 0.8461 - val_loss: 2.4296 - val_acc: 0.6464\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3331 - acc: 0.8537 - val_loss: 1.2301 - val_acc: 0.7234\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3298 - acc: 0.8505 - val_loss: 0.0227 - val_acc: 0.7553\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.3412 - acc: 0.8486 - val_loss: 0.0111 - val_acc: 0.5206\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3313 - acc: 0.8520 - val_loss: 1.0326 - val_acc: 0.7719\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3320 - acc: 0.8513 - val_loss: 1.1602 - val_acc: 0.7480\n",
      "Epoch 223/300\n",
      "200/200 [==============================] - 87s 436ms/step - loss: 0.3331 - acc: 0.8517 - val_loss: 0.0334 - val_acc: 0.7614\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3311 - acc: 0.8532 - val_loss: 1.5395 - val_acc: 0.7112\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.3337 - acc: 0.8499 - val_loss: 0.6260 - val_acc: 0.8055\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3268 - acc: 0.8555 - val_loss: 0.0707 - val_acc: 0.8501\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.3396 - acc: 0.8497 - val_loss: 0.0141 - val_acc: 0.6713\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 86s 430ms/step - loss: 0.3326 - acc: 0.8530 - val_loss: 0.8783 - val_acc: 0.7809\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3372 - acc: 0.8496 - val_loss: 0.0406 - val_acc: 0.7821\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3306 - acc: 0.8537 - val_loss: 0.6288 - val_acc: 0.8148\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3359 - acc: 0.8510 - val_loss: 1.1285 - val_acc: 0.7569\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.3206 - acc: 0.8598 - val_loss: 0.1644 - val_acc: 0.8493\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3241 - acc: 0.8595 - val_loss: 0.4782 - val_acc: 0.8336\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.3212 - acc: 0.8563 - val_loss: 0.1789 - val_acc: 0.8554\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 86s 430ms/step - loss: 0.3297 - acc: 0.8509 - val_loss: 1.7133 - val_acc: 0.7129\n",
      "Epoch 236/300\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.3216 - acc: 0.8547 - val_loss: 0.1324 - val_acc: 0.8586\n",
      "Epoch 237/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 84s 420ms/step - loss: 0.3246 - acc: 0.8535 - val_loss: 0.1205 - val_acc: 0.8609\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.3206 - acc: 0.8587 - val_loss: 0.0934 - val_acc: 0.8426\n",
      "Epoch 239/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.3215 - acc: 0.8577 - val_loss: 0.4200 - val_acc: 0.8450\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 87s 436ms/step - loss: 0.3210 - acc: 0.8562 - val_loss: 1.5565 - val_acc: 0.6827\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.3179 - acc: 0.8631 - val_loss: 1.0044 - val_acc: 0.7608\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.3228 - acc: 0.8612 - val_loss: 1.0850 - val_acc: 0.7664\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3124 - acc: 0.8627 - val_loss: 0.0881 - val_acc: 0.8412\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 87s 433ms/step - loss: 0.3136 - acc: 0.8610 - val_loss: 0.0299 - val_acc: 0.7584\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3071 - acc: 0.8677 - val_loss: 0.5195 - val_acc: 0.8193\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.3157 - acc: 0.8601 - val_loss: 0.0437 - val_acc: 0.7948\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3101 - acc: 0.8656 - val_loss: 0.5143 - val_acc: 0.8296\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3133 - acc: 0.8593 - val_loss: 0.3353 - val_acc: 0.8515\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3226 - acc: 0.8553 - val_loss: 0.6790 - val_acc: 0.8108\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3204 - acc: 0.8578 - val_loss: 0.2289 - val_acc: 0.8567\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3067 - acc: 0.8674 - val_loss: 0.0375 - val_acc: 0.8144\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3104 - acc: 0.8653 - val_loss: 1.3281 - val_acc: 0.7493\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3104 - acc: 0.8640 - val_loss: 0.2248 - val_acc: 0.8619\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3126 - acc: 0.8652 - val_loss: 0.0717 - val_acc: 0.8208\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3062 - acc: 0.8631 - val_loss: 0.4415 - val_acc: 0.8431\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.3025 - acc: 0.8682 - val_loss: 0.7061 - val_acc: 0.8103\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3042 - acc: 0.8677 - val_loss: 0.4535 - val_acc: 0.8412\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3084 - acc: 0.8647 - val_loss: 0.2875 - val_acc: 0.8638\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3012 - acc: 0.8693 - val_loss: 0.0118 - val_acc: 0.6472\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.2965 - acc: 0.8722 - val_loss: 1.7884 - val_acc: 0.7054\n",
      "Epoch 261/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3051 - acc: 0.8667 - val_loss: 1.5216 - val_acc: 0.7269\n",
      "Epoch 262/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.3027 - acc: 0.8676 - val_loss: 0.0967 - val_acc: 0.8475\n",
      "Epoch 263/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.2993 - acc: 0.8700 - val_loss: 0.2085 - val_acc: 0.8629\n",
      "Epoch 264/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.3092 - acc: 0.8641 - val_loss: 0.1324 - val_acc: 0.8629\n",
      "Epoch 265/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3040 - acc: 0.8641 - val_loss: 0.1788 - val_acc: 0.8681\n",
      "Epoch 266/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.3120 - acc: 0.8632 - val_loss: 0.2826 - val_acc: 0.8595\n",
      "Epoch 267/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.3063 - acc: 0.8675 - val_loss: 1.2489 - val_acc: 0.7551\n",
      "Epoch 268/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.2994 - acc: 0.8711 - val_loss: 0.8176 - val_acc: 0.8062\n",
      "Epoch 269/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.3037 - acc: 0.8683 - val_loss: 0.0899 - val_acc: 0.8585\n",
      "Epoch 270/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.3026 - acc: 0.8658 - val_loss: 0.1218 - val_acc: 0.8625\n",
      "Epoch 271/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.3069 - acc: 0.8667 - val_loss: 0.9504 - val_acc: 0.7999\n",
      "Epoch 272/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.2910 - acc: 0.8759 - val_loss: 0.0867 - val_acc: 0.8548\n",
      "Epoch 273/300\n",
      "200/200 [==============================] - 84s 420ms/step - loss: 0.2936 - acc: 0.8701 - val_loss: 0.0795 - val_acc: 0.8534\n",
      "Epoch 274/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.2999 - acc: 0.8683 - val_loss: 0.4972 - val_acc: 0.8404\n",
      "Epoch 275/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.2970 - acc: 0.8669 - val_loss: 0.0987 - val_acc: 0.8691\n",
      "Epoch 276/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.2989 - acc: 0.8692 - val_loss: 0.6908 - val_acc: 0.8154\n",
      "Epoch 277/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.2959 - acc: 0.8696 - val_loss: 0.0880 - val_acc: 0.8626\n",
      "Epoch 278/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.2982 - acc: 0.8708 - val_loss: 1.3600 - val_acc: 0.7440\n",
      "Epoch 279/300\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.2987 - acc: 0.8711 - val_loss: 0.3071 - val_acc: 0.8655\n",
      "Epoch 280/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3033 - acc: 0.8664 - val_loss: 0.3219 - val_acc: 0.8629\n",
      "Epoch 281/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.2964 - acc: 0.8717 - val_loss: 0.2678 - val_acc: 0.8650\n",
      "Epoch 282/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.2966 - acc: 0.8740 - val_loss: 0.8323 - val_acc: 0.8030\n",
      "Epoch 283/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.3001 - acc: 0.8704 - val_loss: 0.6544 - val_acc: 0.8249\n",
      "Epoch 284/300\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.3029 - acc: 0.8694 - val_loss: 0.4952 - val_acc: 0.8466\n",
      "Epoch 285/300\n",
      "200/200 [==============================] - 85s 423ms/step - loss: 0.2983 - acc: 0.8702 - val_loss: 0.4014 - val_acc: 0.8556\n",
      "Epoch 286/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.2914 - acc: 0.8720 - val_loss: 0.4888 - val_acc: 0.8498\n",
      "Epoch 287/300\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.2970 - acc: 0.8685 - val_loss: 0.1823 - val_acc: 0.8706\n",
      "Epoch 288/300\n",
      "200/200 [==============================] - 86s 432ms/step - loss: 0.3002 - acc: 0.8688 - val_loss: 0.4363 - val_acc: 0.8468\n",
      "Epoch 289/300\n",
      "200/200 [==============================] - 86s 429ms/step - loss: 0.2952 - acc: 0.8708 - val_loss: 0.4929 - val_acc: 0.8453\n",
      "Epoch 290/300\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 0.2963 - acc: 0.8735 - val_loss: 0.7575 - val_acc: 0.8131\n",
      "Epoch 291/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.2966 - acc: 0.8726 - val_loss: 0.5476 - val_acc: 0.8370\n",
      "Epoch 292/300\n",
      "200/200 [==============================] - 86s 428ms/step - loss: 0.2875 - acc: 0.8756 - val_loss: 0.4799 - val_acc: 0.8458\n",
      "Epoch 293/300\n",
      "200/200 [==============================] - 85s 426ms/step - loss: 0.2854 - acc: 0.8782 - val_loss: 0.6442 - val_acc: 0.8256\n",
      "Epoch 294/300\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.2945 - acc: 0.8736 - val_loss: 0.3917 - val_acc: 0.8519\n",
      "Epoch 295/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.2954 - acc: 0.8748 - val_loss: 0.3693 - val_acc: 0.8584\n",
      "Epoch 296/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 86s 430ms/step - loss: 0.2899 - acc: 0.8736 - val_loss: 0.6664 - val_acc: 0.8269\n",
      "Epoch 297/300\n",
      "200/200 [==============================] - 84s 419ms/step - loss: 0.2898 - acc: 0.8751 - val_loss: 0.5858 - val_acc: 0.8359\n",
      "Epoch 298/300\n",
      "200/200 [==============================] - 85s 424ms/step - loss: 0.2947 - acc: 0.8716 - val_loss: 0.5871 - val_acc: 0.8397\n",
      "Epoch 299/300\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.2935 - acc: 0.8721 - val_loss: 0.5915 - val_acc: 0.8308\n",
      "Epoch 300/300\n",
      "200/200 [==============================] - 85s 427ms/step - loss: 0.2946 - acc: 0.8737 - val_loss: 0.4107 - val_acc: 0.8549\n"
     ]
    }
   ],
   "source": [
    "# optimizer = SGD(lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = Adam()\n",
    "model_top.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "callback_list = [EarlyStopping(monitor='val_acc', patience=30), \n",
    "                 ReduceLROnPlateau(monitor='loss', factor=np.sqrt(0.5), cooldown=0, patience=5, min_lr=0.5e-5)]\n",
    "output = model_top.fit_generator(ft_gen, steps_per_epoch=200, epochs=300,\n",
    "                                  validation_data=validation_generator, validation_steps=len(validation_generator), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 313/313 [01:42<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_score50 = []\n",
    "output_class50 = []\n",
    "answer_class50 = []\n",
    "answer_class50_1 =[]\n",
    "\n",
    "for i in trange(len(test50_generator)):\n",
    "    output50 = model_top.predict_on_batch(test50_generator[i][0])\n",
    "    output_score50.append(output50)\n",
    "    answer_class50.append(test50_generator[i][1])\n",
    "    \n",
    "output_score50 = np.concatenate(output_score50)\n",
    "answer_class50 = np.concatenate(answer_class50)\n",
    "\n",
    "output_class50 = np.argmax(output_score50, axis=1)\n",
    "answer_class50_1 = np.argmax(answer_class50, axis=1)\n",
    "\n",
    "print(output_class50)\n",
    "print(answer_class50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87     10000\n",
      "           1       0.93      0.78      0.85     10000\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.87      0.86      0.86     20000\n",
      "weighted avg       0.87      0.86      0.86     20000\n",
      "\n",
      "[[9402  598]\n",
      " [2191 7809]]\n",
      "AUROC: 0.947876\n",
      "0.2577655911441234\n",
      "test_acc:  0.86055\n"
     ]
    }
   ],
   "source": [
    "cm50 = confusion_matrix(answer_class50_1, output_class50)\n",
    "report50 = classification_report(answer_class50_1, output_class50)\n",
    "\n",
    "recall50 = cm50[0][0] / (cm50[0][0] + cm50[0][1])\n",
    "fallout50 = cm50[1][0] / (cm50[1][0] + cm50[1][1])\n",
    "\n",
    "fpr50, tpr50, thresholds50 = roc_curve(answer_class50_1, output_score50[:, 1], pos_label=1.)\n",
    "eer50 = brentq(lambda x : 1. - x - interp1d(fpr50, tpr50)(x), 0., 1.)\n",
    "thresh50 = interp1d(fpr50, thresholds50)(eer50)\n",
    "\n",
    "print(report50)\n",
    "print(cm50)\n",
    "print(\"AUROC: %f\" %(roc_auc_score(answer_class50_1, output_score50[:, 1])))\n",
    "print(thresh50)\n",
    "print('test_acc: ', len(output_class50[np.equal(output_class50, answer_class50_1)]) / len(output_class50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top.save('/home/www/fake_detection/model/face2face_shallownet_ft.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
